{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ù‡Ù…Ù‡Ù” Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ Ù¾Ù†Ù‡Ø§Ù† Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# End-to-end HDF vs non-HDF (HRF) pipeline in Python\n",
    "# Jupyter-ready, single-notebook version\n",
    "# ================================================\n",
    "# !pip install pandas numpy scikit-learn imbalanced-learn mygene openpyxl xlsxwriter upsetplot matplotlib joblib tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef, brier_score_loss\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from upsetplot import UpSet, from_contents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------\n",
    "# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ / ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø§Ø±Ø¨Ø±\n",
    "# --------------------------------\n",
    "ROOT_RAW     = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/HRF/Emtahan_dobare_HDF/\"\n",
    "ROOT_PUB     = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/HRF/Publications\"\n",
    "FEATURES_CSV = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/Turtle/Desktop/Masterarbeit/Features/combinedFinal.csv\"\n",
    "VIRUSES      = [\"Denv\",\"IAV\",\"Sars_Cov_2\",\"Zika\"]\n",
    "\n",
    "TOP_N        = 1000\n",
    "TARGET_N     = 1000\n",
    "OUT_SUBDIR   = \"MAIC_top_with_ensembl\"\n",
    "BAL_SUBDIR   = \"balanced_from_bottom\"\n",
    "\n",
    "POS_CLASS    = \"HDF\"   # Ú©Ù„Ø§Ø³ Ù…Ø«Ø¨Øª\n",
    "NEG_CLASS    = \"HRF\"   # non-HDF (Low-MAIC)\n",
    "\n",
    "RANDOM_SEED  = 113\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Ø¨Ø±Ø§ÛŒ cache Ú©Ø±Ø¯Ù† Ù†Ú¯Ø§Ø´Øªâ€ŒÙ‡Ø§ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ù…Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)\n",
    "MAPPING_CACHE = os.path.join(ROOT_RAW, \"_CACHE_symbol2ensembl.csv\")\n",
    "os.makedirs(os.path.dirname(MAPPING_CACHE), exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 0) Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ\n",
    "# ============================================================\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_dirs(p: str) -> List[str]:\n",
    "    return [d for d in sorted(Path(p).iterdir()) if d.is_dir()]\n",
    "\n",
    "def make_safe_sheet_name(name: str) -> str:\n",
    "    n = re.sub(r\"[\\[\\]\\*\\?\\/\\\\:]\", \"_\", name).strip()\n",
    "    return (n[:31] if len(n)>31 else (n if len(n)>0 else \"Sheet\"))\n",
    "\n",
    "def set_global_seeds(seed: int = 113):\n",
    "    np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "def write_compressed_tsv(df: pd.DataFrame, path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        df.to_csv(path, sep=\"\\t\", index=False, compression=\"gzip\")\n",
    "    else:\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Ù†Ú¯Ø§Ø´Øª SYMBOL â†’ ENSEMBL (Ø§ÙˆÙ„ÛŒÙ† ENSG)\n",
    "#    Ø§Ø² mygene Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… + Ú©Ø´ Ù…Ø­Ù„ÛŒ\n",
    "# ============================================================\n",
    "def load_mapping_cache(cache_path=MAPPING_CACHE) -> Dict[str, str]:\n",
    "    if os.path.isfile(cache_path):\n",
    "        m = pd.read_csv(cache_path)\n",
    "        m = m.dropna(subset=[\"symbol\",\"ensembl\"]).drop_duplicates(\"symbol\")\n",
    "        return dict(zip(m[\"symbol\"].astype(str), m[\"ensembl\"].astype(str)))\n",
    "    return {}\n",
    "\n",
    "def save_mapping_cache(map_dict: Dict[str,str], cache_path=MAPPING_CACHE):\n",
    "    if not map_dict:\n",
    "        return\n",
    "    rows = [{\"symbol\":k, \"ensembl\":v} for k,v in map_dict.items() if isinstance(k,str) and isinstance(v,str)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    if os.path.isfile(cache_path):\n",
    "        old = pd.read_csv(cache_path)\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "    df = df.dropna(subset=[\"symbol\",\"ensembl\"]).drop_duplicates(\"symbol\", keep=\"last\")\n",
    "    df.to_csv(cache_path, index=False)\n",
    "\n",
    "def query_mygene_symbols(symbols: List[str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ø§Ø´Øª Ø¨Ø§ mygeneØ› Ø§Ú¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‚Ø·Ø¹ Ø¨Ø§Ø´Ø¯ ÛŒØ§ mygene Ù†ØµØ¨ Ù†Ø¨Ø§Ø´Ø¯ØŒ None Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mygene\n",
    "    except Exception:\n",
    "        return {s: None for s in symbols}\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    out = {s: None for s in symbols}\n",
    "    # batch query\n",
    "    try:\n",
    "        res = mg.querymany(symbols, scopes=\"symbol,alias,ensembl.gene\", fields=\"ensembl.gene\", species=\"human\", as_dataframe=False, returnall=False, verbose=False)\n",
    "    except Exception:\n",
    "        return out\n",
    "    for r in res:\n",
    "        q = r.get('query')\n",
    "        if 'notfound' in r and r['notfound']:\n",
    "            continue\n",
    "        ens = None\n",
    "        # r['ensembl'] Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ dict ÛŒØ§ list Ø¨Ø§Ø´Ø¯\n",
    "        if 'ensembl' in r:\n",
    "            e = r['ensembl']\n",
    "            if isinstance(e, dict) and 'gene' in e:\n",
    "                ens = e['gene']\n",
    "            elif isinstance(e, list) and len(e)>0:\n",
    "                # Ø§ÙˆÙ„ÛŒÙ† gene Ú©Ù‡ Ø¨Ø§ ENSG Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "                for item in e:\n",
    "                    g = item.get('gene') if isinstance(item, dict) else None\n",
    "                    if isinstance(g, str) and g.startswith(\"ENSG\"):\n",
    "                        ens = g; break\n",
    "                if ens is None:\n",
    "                    # fallback Ø¨Ù‡ Ø§ÙˆÙ„ÛŒÙ†\n",
    "                    g = e[0].get('gene') if isinstance(e[0], dict) else None\n",
    "                    ens = g\n",
    "        if isinstance(ens, str) and ens.startswith(\"ENSG\"):\n",
    "            out[q] = ens\n",
    "    return out\n",
    "\n",
    "def map_symbols_to_ensembl_first(symbols: List[str]) -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Ø§ÙˆÙ„ Ø§Ø² Ú©Ø´ØŒ Ø¨Ø¹Ø¯ Ø§Ø² mygene Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. ÙÙ‚Ø· ENSG Ø±Ø§ Ù‚Ø¨ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "    \"\"\"\n",
    "    symbols = [str(s).strip() if s is not None else \"\" for s in symbols]\n",
    "    cache = load_mapping_cache()\n",
    "    result: Dict[str, Optional[str]] = {}\n",
    "    missing = []\n",
    "    for s in symbols:\n",
    "        if s in cache:\n",
    "            result[s] = cache[s]\n",
    "        else:\n",
    "            missing.append(s)\n",
    "    if missing:\n",
    "        qres = query_mygene_symbols(missing)\n",
    "        # Ø¨Ù‡â€ŒØ±ÙˆØ² Ú©Ø±Ø¯Ù† Ú©Ø´ Ø¨Ø§ Ù…ÙˆØ§Ø±Ø¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡\n",
    "        new_pairs = {k:v for k,v in qres.items() if isinstance(v,str) and v.startswith(\"ENSG\")}\n",
    "        if new_pairs:\n",
    "            cache.update(new_pairs)\n",
    "            save_mapping_cache(cache)\n",
    "        result.update(qres)\n",
    "    # ÙÙ‚Ø· ENSGØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª None\n",
    "    out = []\n",
    "    for s in symbols:\n",
    "        v = result.get(s)\n",
    "        out.append(v if isinstance(v,str) and v.startswith(\"ENSG\") else None)\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 2) Ø³Ø§Ø®Øª Top/Bottom Ø¨Ø§ Ensembl Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…\n",
    "# ============================================================\n",
    "def process_one_hdf_file(file_path: str, out_dir: str, top_n: int = 1000):\n",
    "    print(f\">>> Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {file_path}\")\n",
    "    dt = pd.read_csv(file_path, sep=\"\\t\", header=0, dtype=str)\n",
    "    if dt.shape[0] == 0:\n",
    "        print(f\"âš ï¸ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ: {file_path}\"); return\n",
    "    # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
    "    nms = [c.lower() for c in dt.columns]\n",
    "    try:\n",
    "        gene_col = dt.columns[nms.index(\"gene\")]\n",
    "    except ValueError:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ† 'gene' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {file_path}\")\n",
    "    score_col = dt.columns[nms.index(\"maic_score\")] if \"maic_score\" in nms else None\n",
    "\n",
    "    if score_col is not None:\n",
    "        dt[score_col] = pd.to_numeric(dt[score_col], errors=\"coerce\")\n",
    "        dt = dt.sort_values(by=score_col, ascending=False, kind=\"mergesort\")\n",
    "    dt[\"rank_global\"] = np.arange(1, len(dt)+1)\n",
    "\n",
    "    def map_and_write(sub_df: pd.DataFrame, label_suffix: str):\n",
    "        syms = sub_df[gene_col].astype(str).tolist()\n",
    "        ens = map_symbols_to_ensembl_first(syms)\n",
    "        out = sub_df.copy()\n",
    "        out.insert(1, \"ensembl_id\", ens)\n",
    "        removed_na = int(out[\"ensembl_id\"].isna().sum())\n",
    "        out = out.dropna(subset=[\"ensembl_id\"]).copy()\n",
    "\n",
    "        ensure_dir(out_dir)\n",
    "        base_noext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        out_file = os.path.join(out_dir, f\"{base_noext}_{label_suffix}{top_n}_with_ensembl.tsv\")\n",
    "        out.to_csv(out_file, sep=\"\\t\", index=False)\n",
    "        print(f\"    â†³ {label_suffix.upper()}: {len(out)} Ø±Ø¯ÛŒÙ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (Ø­Ø°Ù {removed_na} Ø¨Ø¯ÙˆÙ† Ensembl): {out_file}\")\n",
    "\n",
    "    # Top\n",
    "    top_df = dt.head(min(top_n, len(dt))).copy()\n",
    "    top_df[\"rank_top\"] = np.arange(1, len(top_df)+1)\n",
    "    map_and_write(top_df, \"top\")\n",
    "    # Bottom\n",
    "    bot_df = dt.tail(min(top_n, len(dt))).copy()\n",
    "    bot_df[\"rank_bottom\"] = np.arange(1, len(bot_df)+1)\n",
    "    map_and_write(bot_df, \"bottom\")\n",
    "\n",
    "def step_build_top_bottom():\n",
    "    for virus in VIRUSES:\n",
    "        virus_dir = os.path.join(ROOT_RAW, virus)\n",
    "        if not os.path.isdir(virus_dir):\n",
    "            print(f\"â­ï¸ Ù¾ÙˆØ´Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {virus_dir}\"); continue\n",
    "        print(f\"====== ÙˆÛŒØ±ÙˆØ³: {virus} ======\")\n",
    "        hdf_files = sorted([str(p) for p in Path(virus_dir).glob(\"filtered_H*.txt\")])\n",
    "        if not hdf_files:\n",
    "            print(f\"â­ï¸ filtered_H*.txt ÛŒØ§ÙØª Ù†Ø´Ø¯ Ø¯Ø±: {virus_dir}\"); continue\n",
    "        out_dir = os.path.join(virus_dir, OUT_SUBDIR)\n",
    "        for fp in hdf_files:\n",
    "            process_one_hdf_file(fp, out_dir=out_dir, top_n=TOP_N)\n",
    "    print(\"âœ… Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Top/Bottom ØªÙ…Ø§Ù… Ø´Ø¯.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ HDF/HRF Ø±ÙˆÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ (Top=HDF, Bottom=HRF)\n",
    "# ============================================================\n",
    "def pick_hdf_top_files(virus: str, root: str = ROOT_RAW, subdir: str = OUT_SUBDIR, prefer_all_first=True) -> List[str]:\n",
    "    dir_top = os.path.join(root, virus, subdir)\n",
    "    if not os.path.isdir(dir_top):\n",
    "        raise RuntimeError(f\"Ù¾ÙˆØ´Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {dir_top}\")\n",
    "    files = sorted([str(p) for p in Path(dir_top).glob(\"filtered_HDF_*_top*_with_ensembl.tsv\")])\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ *_top*_with_ensembl.tsv Ø¯Ø± {dir_top} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "    if prefer_all_first:\n",
    "        allc = [f for f in files if re.search(\"All_Categories\", os.path.basename(f), flags=re.I)]\n",
    "        others = [f for f in files if f not in allc]\n",
    "        files = allc + others\n",
    "    return list(dict.fromkeys(files))  # unique, keep order\n",
    "\n",
    "def read_features(features_path: str) -> pd.DataFrame:\n",
    "    ext = Path(features_path).suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(features_path)\n",
    "    elif ext in [\".tsv\",\".txt\"]:\n",
    "        df = pd.read_csv(features_path, sep=\"\\t\")\n",
    "    elif ext == \".rds\" or ext in [\".rdata\",\".rda\"]:\n",
    "        raise RuntimeError(\"ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ RDS/RData Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ø´Ø¯Ù‡â€”Ù„Ø·ÙØ§Ù‹ CSV/TSV Ø¨Ø¯Ù‡ÛŒØ¯.\")\n",
    "    else:\n",
    "        df = pd.read_csv(features_path)  # Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… CSV Ø¨Ø§Ø´Ø¯\n",
    "    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ† Ensembl_ID\n",
    "    cand = [c for c in df.columns if c in [\"Ensembl_ID\",\"ENSEMBL_ID\",\"EnsemblId\",\"ensembl_id\",\"ENSEMBL\",\"Ensembl\",\"ensembl\"]]\n",
    "    if cand:\n",
    "        df = df.rename(columns={cand[0]: \"Ensembl_ID\"})\n",
    "    if \"Ensembl_ID\" not in df.columns:\n",
    "        raise RuntimeError(\"Ø³ØªÙˆÙ† 'Ensembl_ID' Ø¯Ø± ÙÛŒÚ†Ø±Ù‡Ø§ Ù†ÛŒØ³Øª.\")\n",
    "    # ÛŒÚ©ØªØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ ID\n",
    "    df = df.drop_duplicates(subset=[\"Ensembl_ID\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def label_top_bottom_exact_from_raw(hdf_top_file: str, features_path: str, n_each: int = 1000) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    dir_top = os.path.dirname(hdf_top_file)\n",
    "    dir_raw = os.path.dirname(dir_top)\n",
    "    base = re.sub(r\"_top\\d+_with_ensembl\\.tsv$\", \"\", os.path.basename(hdf_top_file), flags=re.I)\n",
    "    raw_guess = os.path.join(dir_raw, f\"{base}.txt\")\n",
    "    if not os.path.isfile(raw_guess):\n",
    "        pat = re.compile(\"^\" + re.escape(base) + r\"\\.txt$\", flags=re.I)\n",
    "        cands = [str(p) for p in Path(dir_raw).glob(\"*.txt\") if pat.search(os.path.basename(p))]\n",
    "        if not cands:\n",
    "            raise RuntimeError(f\"ÙØ§ÛŒÙ„ Ø®Ø§Ù… Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {raw_guess}\")\n",
    "        raw_guess = cands[0]\n",
    "\n",
    "    # Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ MAIC\n",
    "    dt = pd.read_csv(raw_guess, sep=\"\\t\", header=0, dtype=str)\n",
    "    nms = [c.lower() for c in dt.columns]\n",
    "    if \"gene\" not in nms:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ† 'gene' Ø¯Ø± ÙØ§ÛŒÙ„ Ø®Ø§Ù… Ù†ÛŒØ³Øª: {raw_guess}\")\n",
    "    gene_col = dt.columns[nms.index(\"gene\")]\n",
    "    score_col = dt.columns[nms.index(\"maic_score\")] if \"maic_score\" in nms else None\n",
    "\n",
    "    if score_col is not None:\n",
    "        dt[score_col] = pd.to_numeric(dt[score_col], errors=\"coerce\")\n",
    "        dt = dt.sort_values(by=score_col, ascending=False, kind=\"mergesort\")\n",
    "\n",
    "    syms = dt[gene_col].astype(str).tolist()\n",
    "    ens = map_symbols_to_ensembl_first(syms)\n",
    "    dt[\"ensembl_id\"] = ens\n",
    "    dt = dt.dropna(subset=[\"ensembl_id\"]).copy()\n",
    "    ordered_ens = pd.unique(dt[\"ensembl_id\"]).tolist()\n",
    "\n",
    "    feat = read_features(features_path)\n",
    "    present = set(feat[\"Ensembl_ID\"].astype(str))\n",
    "\n",
    "    def pick_n_in_order(vec: List[str], present_ids: set, exclude: set, n_target: int, from_tail: bool=False) -> List[str]:\n",
    "        v = list(reversed(vec)) if from_tail else list(vec)\n",
    "        chosen = []\n",
    "        for g in v:\n",
    "            if g in present_ids and g not in exclude and g not in chosen:\n",
    "                chosen.append(g)\n",
    "                if len(chosen) == n_target:\n",
    "                    break\n",
    "        return list(reversed(chosen)) if from_tail else chosen\n",
    "\n",
    "    top_ids    = pick_n_in_order(ordered_ens, present, set(), n_each, from_tail=False)      # HDF\n",
    "    bottom_ids = pick_n_in_order(ordered_ens, present, set(top_ids), n_each, from_tail=True) # HRF\n",
    "\n",
    "    if len(top_ids)    < n_each: print(f\"âš ï¸ ÙÙ‚Ø· {len(top_ids)} Ø§Ø² {n_each} Ø¢ÛŒØªÙ… Top Ø¯Ø± ÙÛŒÚ†Ø±Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ø´Ø¯.\")\n",
    "    if len(bottom_ids) < n_each: print(f\"âš ï¸ ÙÙ‚Ø· {len(bottom_ids)} Ø§Ø² {n_each} Ø¢ÛŒØªÙ… Bottom Ø¯Ø± ÙÛŒÚ†Ø±Ù‡Ø§ Ù¾ÛŒØ¯Ø§ Ø´Ø¯.\")\n",
    "\n",
    "    feat[\"Class\"] = np.where(feat[\"Ensembl_ID\"].isin(top_ids), POS_CLASS,\n",
    "                      np.where(feat[\"Ensembl_ID\"].isin(bottom_ids), NEG_CLASS, np.nan))\n",
    "    feat_labeled = feat.dropna(subset=[\"Class\"]).copy()\n",
    "\n",
    "    counts = feat_labeled.groupby(\"Class\", dropna=False).size().reset_index(name=\"N\").sort_values(\"N\", ascending=False)\n",
    "    print(counts)\n",
    "    return feat_labeled, counts\n",
    "\n",
    "def step_label_and_save():\n",
    "    for virus in VIRUSES:\n",
    "        print(f\"====== Labeling â†’ {virus} ======\")\n",
    "        try:\n",
    "            top_files = pick_hdf_top_files(virus, root=ROOT_RAW, subdir=OUT_SUBDIR)\n",
    "        except Exception as e:\n",
    "            print(\"â­ï¸\", e); continue\n",
    "        for hdf_file in top_files:\n",
    "            print(f\"â†’ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {os.path.basename(hdf_file)}\")\n",
    "            feat_labeled, counts = label_top_bottom_exact_from_raw(hdf_top_file=hdf_file, features_path=FEATURES_CSV, n_each=TARGET_N)\n",
    "            out_dir = os.path.join(os.path.dirname(hdf_file), BAL_SUBDIR)\n",
    "            ensure_dir(out_dir)\n",
    "            base_stub = re.sub(r\"_top\\d+_with_ensembl\\.tsv$\", \"\", os.path.basename(hdf_file), flags=re.I)\n",
    "            out_csv = os.path.join(out_dir, f\"{base_stub}_bottom{TARGET_N}_HRF__top{TARGET_N}_HDF.csv\")\n",
    "            feat_labeled.to_csv(out_csv, index=False)\n",
    "            print(f\"âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {out_csv}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ML: NZVØŒ Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒØŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
    "# ============================================================\n",
    "def near_zero_var_keep_columns(X: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    ØªÙ‚Ø±ÛŒØ¨ nearZeroVar caret:\n",
    "      - Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙ†ÙˆØ¹ ØµÙØ±\n",
    "      - Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ (freqRatio > 19) Ùˆ (percentUnique <= 10)\n",
    "    \"\"\"\n",
    "    keep = []\n",
    "    n = X.shape[0]\n",
    "    for c in X.columns:\n",
    "        col = X[c].values\n",
    "        # Ø­Ø°Ù ØªÙ…Ø§Ù…-NA\n",
    "        if np.all(pd.isna(col)):\n",
    "            continue\n",
    "        # zero variance\n",
    "        vals, counts = np.unique(col[~pd.isna(col)], return_counts=True)\n",
    "        if len(vals) <= 1:\n",
    "            continue\n",
    "        counts_sorted = np.sort(counts)[::-1]\n",
    "        freq_ratio = counts_sorted[0] / (counts_sorted[1] if len(counts_sorted)>1 else 1)\n",
    "        percent_unique = 100.0 * len(vals) / n\n",
    "        if not (freq_ratio > 19 and percent_unique <= 10):\n",
    "            keep.append(c)\n",
    "    return keep\n",
    "\n",
    "def find_correlation_to_drop(df: pd.DataFrame, threshold: float = 0.70) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ù…Ø¹Ø§Ø¯Ù„ ØªÙ‚Ø±ÛŒØ¨ÛŒ caret::findCorrelation:\n",
    "    Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† |corr| Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ØªØ±Ù†Ø¯ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "    \"\"\"\n",
    "    if df.shape[1] <= 1:\n",
    "        return []\n",
    "    corr = df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = []\n",
    "    while True:\n",
    "        # Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ÙØ¹Ù„ÛŒ\n",
    "        max_corr = (upper.max().max())\n",
    "        if np.isnan(max_corr) or max_corr < threshold:\n",
    "            break\n",
    "        # Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒØ§Ø´ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø³Øª Ø­Ø°Ù Ú©Ù†\n",
    "        mean_corr = upper.mean()\n",
    "        col_to_drop = mean_corr.idxmax()\n",
    "        to_drop.append(col_to_drop)\n",
    "        # Ø­Ø°Ù Ø³ØªÙˆÙ†/Ø³Ø·Ø± Ø§Ø² upper\n",
    "        upper = upper.drop(index=col_to_drop, columns=col_to_drop)\n",
    "    return to_drop\n",
    "\n",
    "def bootstrap_auc_ci(y_true, y_prob, n_boot=1000, seed=113) -> Tuple[float,float,float]:\n",
    "    rng = check_random_state(seed)\n",
    "    aucs = []\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    idx = np.arange(len(y_true))\n",
    "    for _ in range(n_boot):\n",
    "        bs = rng.choice(idx, size=len(idx), replace=True)\n",
    "        try:\n",
    "            a = roc_auc_score(y_true[bs], y_prob[bs])\n",
    "            if not np.isnan(a):\n",
    "                aucs.append(a)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not aucs:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    lo, hi = np.percentile(aucs, [2.5, 97.5])\n",
    "    full_auc = roc_auc_score(y_true, y_prob)\n",
    "    return (lo, full_auc, hi)\n",
    "\n",
    "def train_and_export_single_csv(df_bal: pd.DataFrame, balanced_csv_path: str, pos_class: str = POS_CLASS):\n",
    "    # Ø´Ù†Ø§Ø³Ù‡\n",
    "    id_candidates = [\"Ensembl_ID\",\"ENSEMBL_ID\",\"EnsemblId\",\"ensembl_id\"]\n",
    "    id_col = next((c for c in id_candidates if c in df_bal.columns), None)\n",
    "    if id_col is None:\n",
    "        raise RuntimeError(\"Ø³ØªÙˆÙ† Ø´Ù†Ø§Ø³Ù‡â€ŒÛŒ Ensembl_ID Ø¯Ø± Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "    assert \"Class\" in df_bal.columns\n",
    "\n",
    "    # Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
    "    y = df_bal[\"Class\"].astype(str)\n",
    "    y = pd.Categorical(y, categories=[pos_class, (set([\"HDF\",\"HRF\"]) - {pos_class}).pop()], ordered=True)\n",
    "    y = y.astype(str)\n",
    "\n",
    "    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: Ø¨Ù‚ÛŒÙ‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬Ø² id Ùˆ Class\n",
    "    pred_cols = [c for c in df_bal.columns if c not in [id_col, \"Class\"]]\n",
    "    X_raw = df_bal[pred_cols].copy()\n",
    "\n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…Ù† Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ\n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X_raw, y, df_bal[id_col].astype(str), test_size=0.20, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    # NZV\n",
    "    keep_nzv = near_zero_var_keep_columns(X_train)\n",
    "    if not keep_nzv:\n",
    "        raise RuntimeError(\"[NZV] Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒØ§ÛŒ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.\")\n",
    "    X_train = X_train[keep_nzv].copy()\n",
    "    X_test  = X_test[keep_nzv].copy()\n",
    "\n",
    "    # Preprocess: impute + scale (fit Ø±ÙˆÛŒ Train)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler  = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    X_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test_imp  = pd.DataFrame(imputer.transform(X_test),   columns=X_test.columns,   index=X_test.index)\n",
    "    X_train_scl = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=X_train_imp.columns, index=X_train_imp.index)\n",
    "    X_test_scl  = pd.DataFrame(scaler.transform(X_test_imp),      columns=X_test_imp.columns,  index=X_test_imp.index)\n",
    "\n",
    "    # RFE Ø¨Ø§ RF + CV ØªÚ©Ø±Ø§Ø±ÛŒ\n",
    "    base_rf = RandomForestClassifier(\n",
    "        n_estimators=300, random_state=RANDOM_SEED, n_jobs=-1\n",
    "    )\n",
    "    cv_rfe = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_SEED)\n",
    "    rfe = RFECV(\n",
    "        estimator=base_rf,\n",
    "        step=0.2,\n",
    "        min_features_to_select=1,\n",
    "        cv=cv_rfe,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rfe.fit(X_train_scl, y_train)\n",
    "    sel_mask = rfe.support_\n",
    "    sel_vars = list(X_train_scl.columns[sel_mask])\n",
    "    if not sel_vars:\n",
    "        raise RuntimeError(\"[RFE] Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒØ§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯.\")\n",
    "\n",
    "    # Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø±ÙˆÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡\n",
    "    corr_drop = find_correlation_to_drop(X_train_scl[sel_vars], threshold=0.70)\n",
    "    sel_final = [c for c in sel_vars if c not in corr_drop]\n",
    "    if not sel_final:\n",
    "        sel_final = sel_vars\n",
    "\n",
    "    Xtr_sel = X_train_scl[sel_final].copy()\n",
    "    Xte_sel = X_test_scl[sel_final].copy()\n",
    "\n",
    "    # class imbalance\n",
    "    cls_counts = pd.Series(y_train).value_counts()\n",
    "    imb_ratio = cls_counts.max() / max(1, cls_counts.min()) if len(cls_counts)>=2 else 1.0\n",
    "    use_smote = imb_ratio >= 1.5\n",
    "\n",
    "    # RF tuning (mtry Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ max_features)\n",
    "    p = Xtr_sel.shape[1]\n",
    "    mtries = sorted(set(int(max(1, round(math.sqrt(p)*k))) for k in [0.5,1,2,3]))\n",
    "    param_grid = {\"clf__max_features\": mtries}\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Ø¯Ø± CV Ø§Ø² SMOTE Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø§Ù…Ø§ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø¯ÙˆÙ† SMOTE Ø±ÙˆÛŒ Ú©Ù„ Train ÙÛŒØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "    pipe_cv = ImbPipeline(steps=[\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_SEED)) if use_smote else (\"smote\", \"passthrough\"),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=500, random_state=RANDOM_SEED, n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe_cv,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(Xtr_sel, y_train)\n",
    "    best_mtry = grid.best_params_.get(\"clf__max_features\", None)\n",
    "\n",
    "    # Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¯ÙˆÙ† SMOTEØŒ Ø¨Ø§ best params)\n",
    "    final_rf = RandomForestClassifier(\n",
    "        n_estimators=500, max_features=best_mtry, random_state=RANDOM_SEED, n_jobs=-1\n",
    "    )\n",
    "    final_rf.fit(Xtr_sel, y_train)\n",
    "\n",
    "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Test\n",
    "    pred_cls = final_rf.predict(Xte_sel)\n",
    "    if POS_CLASS not in final_rf.classes_:\n",
    "        raise RuntimeError(\"[PRED_PROB] Ú©Ù„Ø§Ø³ Ù…Ø«Ø¨Øª Ø¯Ø± Ù…Ø¯Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.\")\n",
    "    prob_pos = final_rf.predict_proba(Xte_sel)[:, list(final_rf.classes_).index(POS_CLASS)]\n",
    "\n",
    "    # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§\n",
    "    acc = accuracy_score(y_test, pred_cls)\n",
    "    bal_acc = balanced_accuracy_score(y_test, pred_cls)\n",
    "    try:\n",
    "        auc_val = roc_auc_score((y_test==POS_CLASS).astype(int), prob_pos)\n",
    "    except Exception:\n",
    "        auc_val = np.nan\n",
    "    auc_lo, auc_mid, auc_hi = bootstrap_auc_ci((y_test==POS_CLASS).astype(int), prob_pos, n_boot=1000, seed=RANDOM_SEED)\n",
    "    sens = recall_score(y_test, pred_cls, pos_label=POS_CLASS, zero_division=0)\n",
    "    spec = recall_score(y_test, pred_cls, pos_label=NEG_CLASS, zero_division=0)\n",
    "    prec = precision_score(y_test, pred_cls, pos_label=POS_CLASS, zero_division=0)\n",
    "    reca = sens\n",
    "    f1   = f1_score(y_test, pred_cls, pos_label=POS_CLASS, zero_division=0)\n",
    "    mcc  = matthews_corrcoef(pd.Series(y_test).map({POS_CLASS:1, NEG_CLASS:0}), pd.Series(pred_cls).map({POS_CLASS:1, NEG_CLASS:0}))\n",
    "    brier = brier_score_loss((y_test==POS_CLASS).astype(int), prob_pos)\n",
    "\n",
    "    # Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
    "    base_name = os.path.basename(balanced_csv_path)\n",
    "    base_stub = re.sub(r\"(_balanced_top_vs_bottom|_bottom\\d+_HRF__top\\d+_HDF)\\.csv$\", \"\", base_name, flags=re.I)\n",
    "    out_dir = os.path.join(os.path.dirname(balanced_csv_path), \"ML_results\", base_stub)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # class counts\n",
    "    train_counts = pd.Series(y_train).value_counts().reindex([POS_CLASS, NEG_CLASS]).fillna(0).astype(int)\n",
    "    test_counts  = pd.Series(y_test ).value_counts().reindex([POS_CLASS, NEG_CLASS]).fillna(0).astype(int)\n",
    "    cc = pd.DataFrame({\n",
    "        \"Split\": [\"Train\",\"Train\",\"Test\",\"Test\"],\n",
    "        \"Class\": [POS_CLASS, NEG_CLASS, POS_CLASS, NEG_CLASS],\n",
    "        \"N\":     [train_counts.get(POS_CLASS,0), train_counts.get(NEG_CLASS,0),\n",
    "                  test_counts.get(POS_CLASS,0),  test_counts.get(NEG_CLASS,0)]\n",
    "    })\n",
    "    cc[\"Percent\"] = [round(100*n/cc[cc[\"Split\"]==sp][\"N\"].sum(), 2) for sp,n in zip(cc[\"Split\"], cc[\"N\"])]\n",
    "    cc.to_csv(os.path.join(out_dir, \"class_counts_train_test.csv\"), index=False)\n",
    "\n",
    "    # metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Metric\": [\"N_test\",\"Pos_in_test\",\"Neg_in_test\",\n",
    "                   \"Accuracy\",\"Balanced_Accuracy\",\"AUC\",\"AUC_95CI_Lower\",\"AUC_95CI_Upper\",\n",
    "                   \"Sensitivity\",\"Specificity\",\"Precision\",\"Recall\",\"F1\",\"MCC\",\"Brier_Score\"],\n",
    "        \"Value\":  [Xte_sel.shape[0], int(test_counts.get(POS_CLASS,0)), int(test_counts.get(NEG_CLASS,0)),\n",
    "                   acc, bal_acc, auc_mid, auc_lo, auc_hi,\n",
    "                   sens, spec, prec, reca, f1, mcc, brier]\n",
    "    })\n",
    "    metrics_df.to_csv(os.path.join(out_dir, \"metrics_test.csv\"), index=False)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, pred_cls, labels=[POS_CLASS, NEG_CLASS])\n",
    "    cm_df = pd.DataFrame({\n",
    "        \"Reference\": [POS_CLASS, POS_CLASS, NEG_CLASS, NEG_CLASS],\n",
    "        \"Prediction\":[POS_CLASS, NEG_CLASS, POS_CLASS, NEG_CLASS],\n",
    "        \"Count\": cm.flatten()\n",
    "    })\n",
    "    cm_df.to_csv(os.path.join(out_dir, \"confusion_matrix_test.csv\"), index=False)\n",
    "\n",
    "    # feature importance (Top 30)\n",
    "    importances = final_rf.feature_importances_\n",
    "    imp_df = pd.DataFrame({\"Feature\": sel_final, \"Importance\": importances}).sort_values(\"Importance\", ascending=False)\n",
    "    imp_df.head(30).to_csv(os.path.join(out_dir, \"top_features.csv\"), index=False)\n",
    "\n",
    "    # test predictions\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"Ensembl_ID\": ids_test.values,\n",
    "        \"True_Class\": y_test.values,\n",
    "        \"Pred_Class\": pred_cls,\n",
    "        \"Prob_Pos\":   prob_pos\n",
    "    })\n",
    "    preds_df.to_csv(os.path.join(out_dir, \"test_predictions.csv\"), index=False)\n",
    "\n",
    "    # model summary\n",
    "    model_info = pd.DataFrame({\n",
    "        \"Item\":  [\"Model\",\"Positive_Class\",\"CV_Method\",\"CV_Folds\",\"CV_Repeats\",\"Best_mtry\",\n",
    "                  \"Selected_Features_RFE\",\"Selected_Features_Final\"],\n",
    "        \"Value\": [\"RandomForest\", pos_class, \"repeatedcv\",\"10\",\"3\",\n",
    "                  best_mtry, len(sel_vars), len(sel_final)]\n",
    "    })\n",
    "    model_info.to_csv(os.path.join(out_dir, \"model_summary.csv\"), index=False)\n",
    "\n",
    "    # save lists\n",
    "    pd.DataFrame({\"Feature\": sel_vars}).to_csv(os.path.join(out_dir, \"selected_features_rfe.csv\"), index=False)\n",
    "    pd.DataFrame({\"Feature\": sel_final}).to_csv(os.path.join(out_dir, \"selected_features_final.csv\"), index=False)\n",
    "\n",
    "    # bundle Ø¨Ø±Ø§ÛŒ inference: Ø§Ø¬Ø²Ø§ÛŒ preprocessing + selected vars + Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„\n",
    "    bundle = {\n",
    "        \"imputer\": imputer,           # fitted\n",
    "        \"scaler\": scaler,             # fitted\n",
    "        \"sel_vars\": sel_final,        # Ù„ÛŒØ³Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "        \"all_vars_train\": list(X_train.columns),  # Ù‚Ø¨Ù„ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "        \"id_col\": id_col,\n",
    "        \"pos_class\": pos_class,\n",
    "        \"rf_params\": {\"n_estimators\": 500, \"max_features\": best_mtry, \"random_state\": RANDOM_SEED, \"n_jobs\": -1},\n",
    "        \"created_at\": time.asctime(),\n",
    "        \"version\": {\"python\":\"sklearn-pipeline\", \"seed\": RANDOM_SEED}\n",
    "    }\n",
    "    dump((bundle, final_rf), os.path.join(out_dir, \"model_bundle.joblib\"))\n",
    "    print(f\"âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {out_dir}\")\n",
    "\n",
    "def step_ml_over_balanced_csvs():\n",
    "    for virus in VIRUSES:\n",
    "        bal_dir = os.path.join(ROOT_RAW, virus, OUT_SUBDIR, BAL_SUBDIR)\n",
    "        if not os.path.isdir(bal_dir):\n",
    "            print(f\"â­ï¸ Ù…Ø³ÛŒØ± Ù†ÛŒØ³Øª: {bal_dir}\"); continue\n",
    "        bal_files = sorted([str(p) for p in Path(bal_dir).glob(\"*_bottom*_HRF__top*_HDF.csv\")] +\n",
    "                           [str(p) for p in Path(bal_dir).glob(\"*_balanced_top_vs_bottom.csv\")])\n",
    "        if not bal_files:\n",
    "            print(f\"â­ï¸ ÙØ§ÛŒÙ„ Ø¨Ø§Ù„Ø§Ù†Ø³â€ŒØ´Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± {bal_dir} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\"); continue\n",
    "        print(f\"====== ML â†’ {virus} | files: {len(bal_files)} ======\")\n",
    "        for i, bal_file in enumerate(bal_files, 1):\n",
    "            print(f\"({i}/{len(bal_files)}) Ù¾Ø±Ø¯Ø§Ø²Ø´: {os.path.basename(bal_file)}\")\n",
    "            try:\n",
    "                df_bal = pd.read_csv(bal_file)\n",
    "            except Exception as e:\n",
    "                print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù†:\", e); continue\n",
    "            if \"Class\" not in df_bal.columns:\n",
    "                print(f\"âš ï¸ Ø³ØªÙˆÙ† Class ÛŒØ§ÙØª Ù†Ø´Ø¯: {os.path.basename(bal_file)}\"); continue\n",
    "            if df_bal[\"Class\"].nunique() < 2:\n",
    "                print(f\"âš ï¸ Ú©Ù„Ø§Ø³ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª. Ø±Ø¯ Ø´Ø¯: {os.path.basename(bal_file)}\"); continue\n",
    "            try:\n",
    "                train_and_export_single_csv(df_bal, balanced_csv_path=bal_file, pos_class=POS_CLASS)\n",
    "            except Exception as e:\n",
    "                print(\"âŒ Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ/Ø®Ø±ÙˆØ¬ÛŒ:\", e)\n",
    "\n",
    "    print(\"ğŸ‰ Ù‡Ù…Ù‡Ù” ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§Ù†Ø³â€ŒØ´Ø¯Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯.\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Ø§Ø¯ØºØ§Ù… Ù‡Ù…Ù‡Ù” metrics_test.csv Ø¨Ù‡ long & wide\n",
    "# ============================================================\n",
    "def find_all_ml_results_roots():\n",
    "    roots = []\n",
    "    # Ø§Ø¨ØªØ¯Ø§ ROOT_PUB\n",
    "    for virus in VIRUSES:\n",
    "        base = os.path.join(ROOT_PUB, virus, OUT_SUBDIR, BAL_SUBDIR, \"ML_results\")\n",
    "        if os.path.isdir(base):\n",
    "            roots.append(base)\n",
    "    # Ø³Ù¾Ø³ ROOT_RAW (Ø§Ú¯Ø± Ú©Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¬Ø§Ø¨Ù‡â€ŒØ¬Ø§ Ù†Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯)\n",
    "    for virus in VIRUSES:\n",
    "        base = os.path.join(ROOT_RAW, virus, OUT_SUBDIR, BAL_SUBDIR, \"ML_results\")\n",
    "        if os.path.isdir(base):\n",
    "            roots.append(base)\n",
    "    # unique\n",
    "    return list(dict.fromkeys(roots))\n",
    "\n",
    "def pretty_name(base_stub: str) -> str:\n",
    "    s = re.sub(r\"^filtered_HDF_\", \"\", base_stub, flags=re.I)\n",
    "    s = re.sub(r\"_bottom\\d+_HRF__top\\d+_HDF$\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"_balanced_top_vs_bottom$\", \"\", s, flags=re.I)\n",
    "    return s\n",
    "\n",
    "def step_merge_metrics():\n",
    "    SUMMARY_DIR = os.path.join(ROOT_PUB, \"_ML_SUMMARY\")\n",
    "    ensure_dir(SUMMARY_DIR)\n",
    "\n",
    "    rows_long = []\n",
    "    rows_wide = []\n",
    "\n",
    "    roots = find_all_ml_results_roots()\n",
    "    if not roots:\n",
    "        print(\"âš ï¸ Ù‡ÛŒÚ† Ù¾ÙˆØ´Ù‡Ù” ML_results Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\"); return\n",
    "\n",
    "    for base_dir in roots:\n",
    "        virus_guess = Path(base_dir).parts[-4] if len(Path(base_dir).parts)>=4 else \"NA\"\n",
    "        exp_dirs = [str(p) for p in Path(base_dir).iterdir() if p.is_dir()]\n",
    "        if not exp_dirs:\n",
    "            print(f\"â­ï¸ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ø¯Ø± {base_dir} ÛŒØ§ÙØª Ù†Ø´Ø¯.\"); continue\n",
    "        print(f\"====== Summary â†’ {virus_guess} | experiments: {len(exp_dirs)} ======\")\n",
    "        for exp_dir in exp_dirs:\n",
    "            base_stub = os.path.basename(exp_dir)\n",
    "            exp_name  = pretty_name(base_stub)\n",
    "            f_metrics = os.path.join(exp_dir, \"metrics_test.csv\")\n",
    "            if not os.path.isfile(f_metrics):\n",
    "                print(\"âš ï¸ metrics_test.csv Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª/Ù†Ø§Ù‚Øµ:\", exp_dir); continue\n",
    "            met = pd.read_csv(f_metrics)\n",
    "            if not set([\"Metric\",\"Value\"]).issubset(met.columns):\n",
    "                print(\"âš ï¸ metrics_test.csv Ù†Ø§Ù‚Øµ:\", exp_dir); continue\n",
    "            long_dt = met.copy()\n",
    "            # ØªØ¨Ø¯ÛŒÙ„ Value Ø¨Ù‡ Ø¹Ø¯Ø¯\n",
    "            with np.errstate(all='ignore'):\n",
    "                long_dt[\"Value\"] = pd.to_numeric(long_dt[\"Value\"], errors=\"coerce\")\n",
    "            long_dt[\"Virus\"] = virus_guess\n",
    "            long_dt[\"Experiment\"] = exp_name\n",
    "            long_dt[\"Exp_Path\"] = exp_dir\n",
    "            long_dt = long_dt[[\"Virus\",\"Experiment\",\"Metric\",\"Value\",\"Exp_Path\"]]\n",
    "            rows_long.append(long_dt)\n",
    "\n",
    "            pivot_vals = dict(zip(long_dt[\"Metric\"], long_dt[\"Value\"]))\n",
    "            wide_dt = pd.DataFrame([pivot_vals])\n",
    "            wide_dt[\"Virus\"] = virus_guess\n",
    "            wide_dt[\"Experiment\"] = exp_name\n",
    "            wide_dt[\"Exp_Path\"] = exp_dir\n",
    "            cols = [\"Virus\",\"Experiment\",\"Exp_Path\"] + [c for c in wide_dt.columns if c not in [\"Virus\",\"Experiment\",\"Exp_Path\"]]\n",
    "            wide_dt = wide_dt[cols]\n",
    "            rows_wide.append(wide_dt)\n",
    "\n",
    "    if rows_long:\n",
    "        ALL_LONG = pd.concat(rows_long, ignore_index=True)\n",
    "        ALL_LONG = ALL_LONG.sort_values([\"Virus\",\"Experiment\",\"Metric\"])\n",
    "        out_long = os.path.join(SUMMARY_DIR, \"all_models_metrics_long.csv\")\n",
    "        ALL_LONG.to_csv(out_long, index=False)\n",
    "        print(\"âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:\", out_long)\n",
    "    else:\n",
    "        print(\"âš ï¸ Ù‡ÛŒÚ† metrics_test.csv Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª long Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "\n",
    "    if rows_wide:\n",
    "        ALL_WIDE = pd.concat(rows_wide, ignore_index=True)\n",
    "        ALL_WIDE = ALL_WIDE.sort_values([\"Virus\",\"Experiment\"])\n",
    "        out_wide = os.path.join(SUMMARY_DIR, \"all_models_metrics_wide.csv\")\n",
    "        ALL_WIDE.to_csv(out_wide, index=False)\n",
    "        print(\"âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:\", out_wide)\n",
    "    else:\n",
    "        print(\"âš ï¸ Ù‡ÛŒÚ† metrics_test.csv Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª wide Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Prediction Ø±ÙˆÛŒ Ú©Ù„ Ú˜Ù†ÙˆÙ… + Excel (TopN) + UpSet\n",
    "# ============================================================\n",
    "def gather_model_bundles() -> List[str]:\n",
    "    bundles = []\n",
    "    for root in [ROOT_PUB, ROOT_RAW]:\n",
    "        for virus in VIRUSES:\n",
    "            base = os.path.join(root, virus, OUT_SUBDIR, BAL_SUBDIR, \"ML_results\")\n",
    "            if not os.path.isdir(base): continue\n",
    "            for sd in [p for p in Path(base).iterdir() if p.is_dir()]:\n",
    "                bf = sd / \"model_bundle.joblib\"\n",
    "                if bf.exists():\n",
    "                    bundles.append(str(bf))\n",
    "    return bundles\n",
    "\n",
    "def predict_whole_genome_and_export():\n",
    "    PRED_DIR = os.path.join(ROOT_PUB, \"_PREDICTIONS\")\n",
    "    ensure_dir(PRED_DIR)\n",
    "    XLSX_PATH = os.path.join(PRED_DIR, \"all_models_predictions.xlsx\")\n",
    "    UPSET_PATH= os.path.join(PRED_DIR, \"upset_top1000.png\")\n",
    "\n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙÛŒÚ†Ø±Ù‡Ø§\n",
    "    feat_all = read_features(FEATURES_CSV).copy()\n",
    "    id_col = \"Ensembl_ID\"\n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ù‡Ù…Ù‡Ù” Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± ID Ø¨Ù‡ Ø¹Ø¯Ø¯\n",
    "    num_cols = [c for c in feat_all.columns if c != id_col]\n",
    "    feat_num = feat_all.copy()\n",
    "    for c in num_cols:\n",
    "        feat_num[c] = pd.to_numeric(feat_num[c], errors=\"coerce\")\n",
    "\n",
    "    bundles = gather_model_bundles()\n",
    "    if not bundles:\n",
    "        raise RuntimeError(\"Ù‡ÛŒÚ† model_bundle.joblib Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "\n",
    "    # Excel writer\n",
    "    with pd.ExcelWriter(XLSX_PATH, engine=\"openpyxl\") as writer:\n",
    "        summary_rows = []\n",
    "        top_sets = {}\n",
    "\n",
    "        # ÛŒÚ© Ø´ÛŒØª SUMMARY Ø§Ø² Ø§Ø¨ØªØ¯Ø§\n",
    "        pd.DataFrame({\"INFO\":[\"Predictions summary\"]}).to_excel(writer, sheet_name=\"SUMMARY\", index=False)\n",
    "\n",
    "        for bf in bundles:\n",
    "            bundle, final_rf = load(bf)\n",
    "            req_keys = {\"imputer\",\"scaler\",\"sel_vars\",\"all_vars_train\",\"id_col\",\"pos_class\",\"rf_params\"}\n",
    "            if not req_keys.issubset(set(bundle.keys())):\n",
    "                raise RuntimeError(f\"Ø¨Ø§Ù†Ø¯Ù„ Ù†Ø§Ù‚Øµ Ø§Ø³Øª: {bf}\")\n",
    "\n",
    "            exp_dir = os.path.dirname(bf)\n",
    "            exp_name0 = os.path.basename(exp_dir)\n",
    "            exp_name  = exp_name0\n",
    "            exp_name  = re.sub(r\"^filtered_HDF_\", \"\", exp_name, flags=re.I)\n",
    "            exp_name  = re.sub(r\"_bottom\\d+_HRF__top\\d+_HDF$\", \"\", exp_name, flags=re.I)\n",
    "            exp_name  = re.sub(r\"_balanced_top_vs_bottom$\", \"\", exp_name, flags=re.I)\n",
    "            sheet = make_safe_sheet_name(exp_name)\n",
    "\n",
    "            pre_cols = bundle[\"all_vars_train\"]\n",
    "            missing = [c for c in pre_cols if c not in feat_num.columns]\n",
    "            if missing:\n",
    "                raise RuntimeError(f\"Ø§ÛŒÙ† ÙÛŒÚ†Ø±Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ ÙÛŒÚ†Ø±Ù‡Ø§ Ù†ÛŒØ³ØªÙ†Ø¯ ({len(missing)}): {', '.join(missing[:10])} ...\")\n",
    "\n",
    "            # preprocess\n",
    "            X_full_raw = feat_num[pre_cols].copy()\n",
    "            X_full_imp = pd.DataFrame(bundle[\"imputer\"].transform(X_full_raw), columns=pre_cols)\n",
    "            X_full_scl = pd.DataFrame(bundle[\"scaler\"].transform(X_full_imp), columns=pre_cols)\n",
    "\n",
    "            sel_vars = bundle[\"sel_vars\"] if bundle.get(\"sel_vars\") else pre_cols\n",
    "            for c in sel_vars:\n",
    "                if c not in X_full_scl.columns:\n",
    "                    raise RuntimeError(f\"selected_vars Ø¯Ø± Ø¯Ø§Ø¯Ù‡Ù” Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ù†ÛŒØ³Øª: {c}\")\n",
    "\n",
    "            X_pred = X_full_scl[sel_vars].copy()\n",
    "            if bundle[\"pos_class\"] not in final_rf.classes_:\n",
    "                raise RuntimeError(\"[PRED_PROB] Ú©Ù„Ø§Ø³ Ù…Ø«Ø¨Øª Ø¯Ø± Ù…Ø¯Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.\")\n",
    "            prob_pos = final_rf.predict_proba(X_pred)[:, list(final_rf.classes_).index(bundle[\"pos_class\"])]\n",
    "\n",
    "            res = pd.DataFrame({\n",
    "                \"Ensembl_ID\": feat_all[id_col].astype(str).values,\n",
    "                \"Prob_HDF\": prob_pos\n",
    "            })\n",
    "            res = res.sort_values([\"Prob_HDF\",\"Ensembl_ID\"], ascending=[False, True]).reset_index(drop=True)\n",
    "            res[\"Rank\"] = np.arange(1, len(res)+1)\n",
    "\n",
    "            tsv_path = os.path.join(PRED_DIR, f\"{sheet}_predictions.tsv.gz\")\n",
    "            res.to_csv(tsv_path, sep=\"\\t\", index=False, compression=\"gzip\")\n",
    "\n",
    "            # Excel: ÙÙ‚Ø· TopN (Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶)\n",
    "            res.head(1000).to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "            summary_rows.append({\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Sheet\": sheet,\n",
    "                \"ModelBundle\": bf,\n",
    "                \"TSV_GZ\": tsv_path,\n",
    "                \"N_all\": len(res),\n",
    "                \"TopN_in_Excel\": min(1000, len(res)),\n",
    "                \"Top1000_MinProb\": (res[\"Prob_HDF\"].iloc[999] if len(res)>=1000 else np.nan)\n",
    "            })\n",
    "            top_sets[sheet] = set(res.head(1000)[\"Ensembl_ID\"].tolist())\n",
    "\n",
    "        # SUMMARY sheet\n",
    "        if summary_rows:\n",
    "            SUM = pd.DataFrame(summary_rows)\n",
    "            # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ SUMMARY\n",
    "            workbook = writer.book\n",
    "            if \"SUMMARY\" in writer.sheets:\n",
    "                std = writer.sheets[\"SUMMARY\"]\n",
    "                # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ùˆ Ù†ÙˆØ´ØªÙ† Ù…Ø¬Ø¯Ø¯\n",
    "                # (Ø³Ø§Ø¯Ù‡â€ŒØªØ±: Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØª Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù†Ø§Ù… SUMMARY2)\n",
    "            SUM.to_excel(writer, sheet_name=\"SUMMARY\", index=False)\n",
    "\n",
    "    print(f\"âœ… Excel saved: {XLSX_PATH}\")\n",
    "\n",
    "    # UpSet plot\n",
    "    if len(top_sets) >= 2:\n",
    "        contents = {k:list(v) for k,v in top_sets.items()}\n",
    "        inc = from_contents(contents)\n",
    "        plt.figure(figsize=(12,7))\n",
    "        UpSet(inc, subset_size='count', show_counts=True).plot()\n",
    "        plt.suptitle(\"Top-1000 Overlaps (Predicted HDFs)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(UPSET_PATH, dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"âœ… UpSet saved: {UPSET_PATH}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ UpSet Ø³Ø§Ø®ØªÙ‡ Ù†Ø´Ø¯ (Ú©Ù…ØªØ± Ø§Ø² Û² Ù…Ø¯Ù„).\")\n",
    "\n",
    "# ============================================================\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† (Ø¯Ø± ØµÙˆØ±Øª ØªÙ…Ø§ÛŒÙ„ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯)\n",
    "# ============================================================\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 1: Ø³Ø§Ø®Øª Top/Bottom Ø¨Ø§ Ù†Ú¯Ø§Ø´Øª Ensembl\n",
    "#step_build_top_bottom()\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ HDF/HRF Ø±ÙˆÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ Ùˆ Ø³Ø§Ø®Øª CSV Ø¨Ø§Ù„Ø§Ù†Ø³\n",
    "#step_label_and_save()\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 3: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø±ÙˆÛŒ CSVÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§Ù†Ø³â€ŒØ´Ø¯Ù‡ + Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
    "# step_ml_over_balanced_csvs()\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 4: Ø§Ø¯ØºØ§Ù… Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ù¾ÙˆØ´Ù‡ Summary\n",
    "# step_merge_metrics()\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 5: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ú©Ù„ Ú˜Ù†ÙˆÙ… + Excel + UpSet\n",
    "# predict_whole_genome_and_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With saving Hyperparameter:\n",
    "# ==========================================\n",
    "# han_hdf_han_final_cv.py  (Final, inductive option) â€” Nested CV + Tuning CSV\n",
    "# - Inductive training/eval pipeline [INDUCTIVE]\n",
    "# - Enforce Ï†4 (H->V->V->H) usage\n",
    "# - Uses virus features via token fusion\n",
    "# - REAL nested CV for hyperparameter tuning (leakage-safe)\n",
    "# - Writes per-fold tuning summary + consensus CSV for your paper tables\n",
    "# ==========================================\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from typing import Tuple, Optional, Dict, List, Tuple as Tup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             precision_score, recall_score, f1_score, accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config / Paths\n",
    "# -----------------------------\n",
    "PATH  = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/GNN/Zika/\"\n",
    "PATH2 = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/Zika/\"\n",
    "\n",
    "FILE_GV       = os.path.join(PATH,  \"InteractionData_Zika_Human_For_GNN.xlsx\")      # HV edges\n",
    "FILE_GENE     = os.path.join(PATH2, \"ZikaInputdataForDeepL500HDF1000Non39Features_WithClass.csv\")  # gene feats + Class\n",
    "FILE_PPI_H    = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/GNN/BiogridHuman-With_EID.csv\"  # HH\n",
    "FILE_VIRUS    = os.path.join(PATH,  \"GeneVirus_Zika.xlsx\")                           # virus feats\n",
    "FILE_PPI_VV   = os.path.join(PATH,  \"Zika_Zika_Pr_Pr_interaction.xlsx\")              # VV\n",
    "FILE_GENE_ALL = os.path.join(PATH2, \"Zika_normalized_whole_data_withoutclass_500HDF1000Non39Features.csv\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------- reporting / virus name --------\n",
    "VIRUS_NAME = \"ZIKV\"   # Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒØ±ÙˆØ³ Ù…Ù‚Ø¯Ø§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø¨Ú¯Ø°Ø§Ø±\n",
    "TUNING_SUMMARY_PATH = f\"tuning_summary_{VIRUS_NAME}.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Training & model\n",
    "# -----------------------------\n",
    "HIDDEN  = 256\n",
    "HEADS   = 4\n",
    "DROPOUT = 0.30\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR = 1e-3\n",
    "MAX_EPOCHS = 300\n",
    "PATIENCE = 30\n",
    "N_SPLITS = 10\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# ØªØ¹Ø¯Ø§Ø¯ ÙÙˆÙ„Ø¯ Ø¨Ø±Ø§ÛŒ Ù†ÙØ³ØªØ¯ (inner) CV\n",
    "K_INNER = 5\n",
    "\n",
    "# DropEdge Ø±ÙˆÛŒ ÛŒØ§Ù„â€ŒÙ‡Ø§ÛŒ Ï† Ø¯Ø± Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´\n",
    "DROPEDGE_PHI_P = 0.20\n",
    "\n",
    "# Path-guided budgets (per-hop)\n",
    "K_HH_PHI2 = 10      # H->H (Ï†2)\n",
    "K_HH_PHI3 = 5       # H->H Ø¯Ø± Ú¯Ø§Ù… Ø§ÙˆÙ„ Ï†3\n",
    "K_HV_GENE_TO_V = None   # H->V (Ï†1/Ï†3): None -> Ù‡Ù…Ù‡\n",
    "K_VH_VIRUS_TO_H = 15    # V->H (Ï†1/Ï†3)\n",
    "K_VV_PER_VIRUS  = 10    # V->V (Ï†4)\n",
    "\n",
    "# VV control\n",
    "INCLUDE_VV_METAPATH   = True   # enforce building Ï†4\n",
    "MIN_VV_EDGES_FOR_USE  = 1      # require even minimal VV to allow Ï†4\n",
    "\n",
    "# Post-HAN MHA (Feature Fusion)\n",
    "USE_POST_MHA = True\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def make_binary_labels(class_series: pd.Series) -> torch.Tensor:\n",
    "    s = class_series.astype(str).str.strip()\n",
    "    labels_map = {'HDF': 1, 'nonHDF': 0, 'NonHDF': 0, 'nonHdf': 0, 'NONHDF': 0}\n",
    "    if set(s.unique()).issubset(labels_map.keys()):\n",
    "        y = s.map(labels_map).astype(int).values\n",
    "        return torch.tensor(y, dtype=torch.long)\n",
    "    s_num = pd.to_numeric(s, errors='coerce')\n",
    "    if s_num.isna().any():\n",
    "        bad = class_series[s_num.isna()].head(10)\n",
    "        raise ValueError(f\"Unknown labels in 'Class'. Examples:\\n{bad}\")\n",
    "    un = set(s_num.astype(int).unique())\n",
    "    if un.issubset({0,1}):\n",
    "        return torch.tensor(s_num.astype(int).values, dtype=torch.long)\n",
    "    if un == {1,2}:\n",
    "        return torch.tensor(s_num.replace({1:1,2:0}).astype(int).values, dtype=torch.long)\n",
    "    raise ValueError(f\"Unsupported label set: {sorted(list(un))}\")\n",
    "\n",
    "def topk_per_src(edge_index: torch.Tensor, num_src: int, k: Optional[int], seed: int = SEED) -> torch.Tensor:\n",
    "    if edge_index.numel() == 0 or k is None or k <= 0:\n",
    "        return edge_index\n",
    "    rng = np.random.default_rng(seed)\n",
    "    row, col = edge_index.detach().cpu().numpy()\n",
    "    nbrs: Dict[int, List[int]] = {}\n",
    "    for u, v in zip(row, col):\n",
    "        nbrs.setdefault(u, []).append(v)\n",
    "    new_row, new_col = [], []\n",
    "    for u in range(num_src):\n",
    "        vs = nbrs.get(u, [])\n",
    "        if len(vs) > k:\n",
    "            vs = rng.choice(vs, size=k, replace=False)\n",
    "        for v in vs:\n",
    "            new_row.append(u); new_col.append(v)\n",
    "    if len(new_row) == 0:\n",
    "        return edge_index.new_zeros((2,0))\n",
    "    return torch.tensor([new_row, new_col], dtype=torch.long, device=edge_index.device)\n",
    "\n",
    "def unique_edges(e: torch.Tensor) -> torch.Tensor:\n",
    "    if e.numel() == 0:\n",
    "        return e\n",
    "    u = torch.unique(e.t().contiguous(), dim=0)\n",
    "    return u.t().contiguous()\n",
    "\n",
    "def compose_edges(edge_ab: torch.Tensor, edge_bc: torch.Tensor,\n",
    "                  num_a: int, num_b: int) -> torch.Tensor:\n",
    "    if edge_ab.numel() == 0 or edge_bc.numel() == 0:\n",
    "        return edge_ab.new_zeros((2,0), dtype=torch.long)\n",
    "    b2c: Dict[int, List[int]] = {}\n",
    "    b, c = edge_bc[0].cpu().numpy(), edge_bc[1].cpu().numpy()\n",
    "    for bb, cc in zip(b, c):\n",
    "        b2c.setdefault(int(bb), []).append(int(cc))\n",
    "    a_list, c_list = [], []\n",
    "    a_arr, b_arr = edge_ab[0].cpu().numpy(), edge_ab[1].cpu().numpy()\n",
    "    for aa, bb in zip(a_arr, b_arr):\n",
    "        outs = b2c.get(int(bb), [])\n",
    "        for cc in outs:\n",
    "            a_list.append(int(aa)); c_list.append(int(cc))\n",
    "    if len(a_list) == 0:\n",
    "        return edge_ab.new_zeros((2,0), dtype=torch.long)\n",
    "    e = torch.tensor([a_list, c_list], dtype=torch.long, device=edge_ab.device)\n",
    "    mask = (e[0] != e[1])\n",
    "    e = e[:, mask]\n",
    "    e = unique_edges(e)\n",
    "    return e\n",
    "\n",
    "# ---- Virus aggregation: mean of neighbor viruses per gene ----\n",
    "def aggregate_virus_mean(x_virus: torch.Tensor, hv_edge: torch.Tensor, num_genes: int) -> torch.Tensor:\n",
    "    if hv_edge.numel() == 0:\n",
    "        return x_virus.new_zeros((num_genes, x_virus.size(1)))\n",
    "    gene_idx = hv_edge[0]\n",
    "    virus_idx = hv_edge[1]\n",
    "    d = x_virus.size(1)\n",
    "    agg = x_virus.new_zeros((num_genes, d))\n",
    "    agg.index_add_(0, gene_idx, x_virus[virus_idx])\n",
    "    deg = torch.bincount(gene_idx, minlength=num_genes).clamp(min=1).unsqueeze(1).to(agg.dtype)\n",
    "    return agg / deg\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Build base hetero + budgets + compose Ï†-edges\n",
    "# -----------------------------\n",
    "def build_base_heterodata(apply_undirected: bool = True) -> Tuple[HeteroData, pd.DataFrame, pd.DataFrame]:\n",
    "    interactions_df = pd.read_excel(FILE_GV)\n",
    "    features_df     = pd.read_csv(FILE_GENE)\n",
    "    biogrid_df      = pd.read_csv(FILE_PPI_H)\n",
    "    virus_df        = pd.read_excel(FILE_VIRUS)\n",
    "    vv_df           = pd.read_excel(FILE_PPI_VV)\n",
    "\n",
    "    for df, req in [(interactions_df, ['Genes','GenesVirus']),\n",
    "                    (biogrid_df, ['Ensembl_ID_A','Ensembl_ID_B']),\n",
    "                    (virus_df, ['GenesSymbolVirus']),\n",
    "                   ]:\n",
    "        miss = [c for c in req if c not in df.columns]\n",
    "        if miss:\n",
    "            raise KeyError(f\"Missing columns {miss} in dataframe with columns {list(df.columns)}\")\n",
    "\n",
    "    y_gene = make_binary_labels(features_df['Class'])\n",
    "\n",
    "    interactions_df = interactions_df[\n",
    "        (interactions_df['Genes'].isin(features_df['Genes'])) &\n",
    "        (interactions_df['GenesVirus'].isin(virus_df['GenesSymbolVirus']))\n",
    "    ]\n",
    "    biogrid_df = biogrid_df[\n",
    "        (biogrid_df['Ensembl_ID_A'].isin(features_df['Genes'])) &\n",
    "        (biogrid_df['Ensembl_ID_B'].isin(features_df['Genes']))\n",
    "    ]\n",
    "\n",
    "    genes_to_index  = {g: i for i, g in enumerate(features_df['Genes'])}\n",
    "    # ---- FIXED LINE ----\n",
    "    virus_to_index = {v: i for i, v in enumerate(virus_df['GenesSymbolVirus'])}\n",
    "\n",
    "    X_gene  = torch.tensor(features_df.iloc[:, 1:-1].values, dtype=torch.float)\n",
    "    X_virus = torch.tensor(virus_df.iloc[:, 1:].values,       dtype=torch.float)\n",
    "\n",
    "    gv_edges = torch.tensor([\n",
    "        [genes_to_index[row['Genes']], virus_to_index[row['GenesVirus']]]\n",
    "        for _, row in interactions_df.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    hh_edges = torch.tensor([\n",
    "        [genes_to_index[row['Ensembl_ID_A']], genes_to_index[row['Ensembl_ID_B']]]\n",
    "        for _, row in biogrid_df.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    a_col = 'Official_Symbol_A_Zika'; b_col = 'Official_Symbol_B_Zika'\n",
    "    if a_col in vv_df.columns and b_col in vv_df.columns:\n",
    "        vv_edges = torch.tensor([\n",
    "            [virus_to_index.get(row[a_col], -1), virus_to_index.get(row[b_col], -1)]\n",
    "            for _, row in vv_df.iterrows()\n",
    "        ], dtype=torch.long).t().contiguous()\n",
    "        if vv_edges.numel() > 0:\n",
    "            mask_valid = (vv_edges >= 0).all(dim=0)\n",
    "            vv_edges = vv_edges[:, mask_valid]\n",
    "    else:\n",
    "        vv_edges = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data['gene'].x  = X_gene\n",
    "    data['gene'].y  = y_gene\n",
    "    data['virus'].x = X_virus\n",
    "    data['gene','interacts','virus'].edge_index = gv_edges\n",
    "    data['gene','interacts','gene' ].edge_index = hh_edges\n",
    "    if vv_edges.numel() > 0:\n",
    "        data['virus','interacts','virus'].edge_index = vv_edges\n",
    "\n",
    "    if apply_undirected:\n",
    "        data = T.ToUndirected()(data)  # creates 'rev_interacts'\n",
    "    return data, features_df, virus_df\n",
    "\n",
    "def add_budget_relations(data: HeteroData) -> HeteroData:\n",
    "    Ng = data['gene'].x.size(0)\n",
    "    Nv = data['virus'].x.size(0)\n",
    "\n",
    "    hv_key = ('gene','interacts','virus')\n",
    "    vh_key = ('virus','rev_interacts','gene') if ('virus','rev_interacts','gene') in data.edge_index_dict else ('virus','interacts','gene')\n",
    "    hh_key = ('gene','interacts','gene')\n",
    "    vv_key = ('virus','interacts','virus')\n",
    "\n",
    "    if hh_key in data.edge_index_dict and data[hh_key].edge_index.numel() > 0:\n",
    "        hh_e = data[hh_key].edge_index\n",
    "        data['gene','hh2','gene'].edge_index = topk_per_src(hh_e, Ng, K_HH_PHI2, seed=SEED)\n",
    "        data['gene','hh3','gene'].edge_index = topk_per_src(hh_e, Ng, K_HH_PHI3, seed=SEED+1)\n",
    "\n",
    "    if hv_key in data.edge_index_dict and data[hv_key].edge_index.numel() > 0:\n",
    "        hv_e = data[hv_key].edge_index\n",
    "        data['gene','hv','virus'].edge_index = topk_per_src(hv_e, Ng, K_HV_GENE_TO_V, seed=SEED+2)\n",
    "\n",
    "    if vh_key in data.edge_index_dict and data[vh_key].edge_index.numel() > 0:\n",
    "        vh_e = data[vh_key].edge_index\n",
    "        data['virus','vh','gene'].edge_index = topk_per_src(vh_e, Nv, K_VH_VIRUS_TO_H, seed=SEED+3)\n",
    "\n",
    "    if vv_key in data.edge_index_dict and data[vv_key].edge_index.numel() > 0:\n",
    "        vv_e = data[vv_key].edge_index\n",
    "        data['virus','vv','virus'].edge_index = topk_per_src(vv_e, Nv, K_VV_PER_VIRUS, seed=SEED+4)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_metapath_phi_edges(data: HeteroData, include_vv: bool = INCLUDE_VV_METAPATH,\n",
    "                           min_vv_edges: int = MIN_VV_EDGES_FOR_USE) -> HeteroData:\n",
    "    Ng = data['gene'].x.size(0); Nv = data['virus'].x.size(0)\n",
    "    device = data['gene'].x.device\n",
    "\n",
    "    hv = ('gene','hv','virus'); vh = ('virus','vh','gene')\n",
    "    hh2 = ('gene','hh2','gene'); hh3 = ('gene','hh3','gene'); vv = ('virus','vv','virus')\n",
    "\n",
    "    def get_e(key):\n",
    "        return data[key].edge_index if key in data.edge_index_dict else torch.empty((2,0), dtype=torch.long, device=device)\n",
    "\n",
    "    hv_e  = get_e(hv)\n",
    "    vh_e  = get_e(vh)\n",
    "    hh2_e = get_e(hh2)\n",
    "    hh3_e = get_e(hh3)\n",
    "    vv_e  = get_e(vv)\n",
    "\n",
    "    if hh2_e.numel() > 0:\n",
    "        data['gene','phi2','gene'].edge_index = unique_edges(hh2_e)\n",
    "\n",
    "    if hv_e.numel() > 0 and vh_e.numel() > 0:\n",
    "        phi1 = compose_edges(hv_e, vh_e, Ng, Nv)\n",
    "        if phi1.numel() > 0:\n",
    "            data['gene','phi1','gene'].edge_index = phi1\n",
    "\n",
    "    if hh3_e.numel() > 0 and hv_e.numel() > 0 and vh_e.numel() > 0:\n",
    "        tmp = compose_edges(hh3_e, hv_e, Ng, Ng)\n",
    "        phi3 = compose_edges(tmp, vh_e, Ng, Nv)\n",
    "        if phi3.numel() > 0:\n",
    "            data['gene','phi3','gene'].edge_index = phi3\n",
    "\n",
    "    # Ï†4 (MANDATORY) = H->V->V->H\n",
    "    if include_vv:\n",
    "        if vv_e.numel() < min_vv_edges:\n",
    "            raise RuntimeError(\"Ï†4 required: insufficient V-V edges for composing Hâ†’Vâ†’Vâ†’H.\")\n",
    "        if hv_e.numel() == 0 or vh_e.numel() == 0:\n",
    "            raise RuntimeError(\"Ï†4 required: missing Hâ†’V or Vâ†’H edges for composing Hâ†’Vâ†’Vâ†’H.\")\n",
    "        tmp = compose_edges(hv_e, vv_e, Ng, Nv)\n",
    "        phi4 = compose_edges(tmp, vh_e, Ng, Nv)\n",
    "        if phi4.numel() == 0:\n",
    "            raise RuntimeError(\"Ï†4 required: composition produced zero edges for Hâ†’Vâ†’Vâ†’H.\")\n",
    "        data['gene','phi4','gene'].edge_index = phi4\n",
    "\n",
    "    # fallback\n",
    "    if not any((('gene',f'phi{i}','gene') in data.edge_index_dict and data[('gene',f'phi{i}','gene')].edge_index.numel() > 0) for i in [1,2,3,4]):\n",
    "        if ('gene','interacts','gene') in data.edge_index_dict:\n",
    "            data['gene','phi2','gene'].edge_index = unique_edges(data['gene','interacts','gene'].edge_index)\n",
    "        else:\n",
    "            raise RuntimeError(\"No meta-path edges could be constructed.\")\n",
    "    return data\n",
    "\n",
    "def build_hdf_heterodata_with_phi() -> Tuple[HeteroData, pd.DataFrame, pd.DataFrame]:\n",
    "    data, features_df, virus_df = build_base_heterodata(apply_undirected=True)\n",
    "    data = add_budget_relations(data)\n",
    "    data = add_metapath_phi_edges(data, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "    return data, features_df, virus_df\n",
    "\n",
    "# -----------------------------\n",
    "# [INDUCTIVE] - build induced graphs by allowed genes\n",
    "# -----------------------------\n",
    "def induce_graph_by_genes(data_base: HeteroData, allowed_gene_idx: np.ndarray) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Create a graph where only edges incident to allowed genes (for gene-gene and gene-virus directions) are kept.\n",
    "    VV edges are kept as-is. Budgets and Ï† will be rebuilt on the filtered graph.\n",
    "    \"\"\"\n",
    "    data = copy.deepcopy(data_base)\n",
    "    Ng = data['gene'].x.size(0)\n",
    "    allowed_mask = torch.zeros(Ng, dtype=torch.bool)\n",
    "    allowed_mask[torch.as_tensor(allowed_gene_idx, dtype=torch.long)] = True\n",
    "\n",
    "    # remove any previous budget/phi if present\n",
    "    for k in list(data.edge_index_dict.keys()):\n",
    "        if k[0]=='gene' and k[2]=='gene' and (k[1].startswith('phi') or k[1] in ['hh2','hh3']):\n",
    "            del data[k]\n",
    "        if k == ('gene','hv','virus') or k == ('virus','vh','gene') or (k[0]=='virus' and k[2]=='virus' and k[1]=='vv'):\n",
    "            del data[k]\n",
    "\n",
    "    # filter base HH\n",
    "    hh_key = ('gene','interacts','gene')\n",
    "    if hh_key in data.edge_index_dict:\n",
    "        e = data[hh_key].edge_index\n",
    "        keep = allowed_mask[e[0]] & allowed_mask[e[1]]\n",
    "        data[hh_key].edge_index = e[:, keep]\n",
    "\n",
    "    # filter HV and VH (both directions)\n",
    "    hv_key = ('gene','interacts','virus')\n",
    "    if hv_key in data.edge_index_dict:\n",
    "        e = data[hv_key].edge_index\n",
    "        keep = allowed_mask[e[0]]\n",
    "        data[hv_key].edge_index = e[:, keep]\n",
    "\n",
    "    vh_key = ('virus','rev_interacts','gene') if ('virus','rev_interacts','gene') in data.edge_index_dict else ('virus','interacts','gene')\n",
    "    if vh_key in data.edge_index_dict:\n",
    "        e = data[vh_key].edge_index\n",
    "        keep = allowed_mask[e[1]]\n",
    "        data[vh_key].edge_index = e[:, keep]\n",
    "\n",
    "    # rebuild budgets + Ï† on filtered graph\n",
    "    data = add_budget_relations(data)\n",
    "    data = add_metapath_phi_edges(data, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "    return data\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Semantic Attention (meta-path level)\n",
    "# -----------------------------\n",
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, dim, hidden=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor, mask: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        scores = self.proj(X).squeeze(-1)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e4)\n",
    "        alpha = torch.softmax(scores, dim=1)\n",
    "        fused = torch.sum(alpha.unsqueeze(-1) * X, dim=1)\n",
    "        return fused, alpha\n",
    "\n",
    "# -----------------------------\n",
    "# Post-HAN Feature Fusion (optional)\n",
    "# -----------------------------\n",
    "class FeatureFusionMHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.post = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        N = tokens.size(0)\n",
    "        cls = self.cls.expand(N, 1, -1)\n",
    "        z, _ = self.mha(cls, tokens, tokens)\n",
    "        z = self.post(z)\n",
    "        return z.squeeze(1)\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "class SimpleHAN(nn.Module):\n",
    "    \"\"\"\n",
    "    - Per Ï†: GAT x2 (gene->gene)\n",
    "    - Semantic attention across Ï†\n",
    "    - Fusion via MHA over tokens: [proj_gene, han_fused, virus_token]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim_gene: int, in_dim_virus: int,\n",
    "                 phi_names: List[str], hidden: int = HIDDEN, heads: int = HEADS, dropout: float = DROPOUT,\n",
    "                 use_post_mha: bool = USE_POST_MHA, use_virus_token: bool = True):\n",
    "        super().__init__()\n",
    "        self.phi_names = phi_names\n",
    "        self.use_post_mha = use_post_mha\n",
    "        self.use_virus_token = use_virus_token\n",
    "        self.hidden = hidden\n",
    "        self.heads = heads\n",
    "\n",
    "        self.proj_gene  = nn.Sequential(nn.Linear(in_dim_gene,  hidden), nn.ReLU(), nn.Dropout(dropout))\n",
    "        self.proj_virus = nn.Sequential(nn.Linear(in_dim_virus, hidden), nn.ReLU(), nn.Dropout(dropout))\n",
    "\n",
    "        self.gat1 = nn.ModuleDict({phi: GATConv(hidden, hidden, heads=heads, concat=False, dropout=dropout) for phi in phi_names})\n",
    "        self.gat2 = nn.ModuleDict({phi: GATConv(hidden, hidden, heads=heads, concat=False, dropout=dropout) for phi in phi_names})\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden)\n",
    "        self.norm2 = nn.LayerNorm(hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.semantic_att = SemanticAttention(hidden, hidden=128, dropout=dropout)\n",
    "\n",
    "        if self.use_post_mha:\n",
    "            self.post_mha = FeatureFusionMHA(hidden, n_heads=heads, dropout=dropout)\n",
    "\n",
    "        self.cls = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        xg = self.proj_gene(x_dict['gene'])\n",
    "        xv = self.proj_virus(x_dict['virus'])\n",
    "\n",
    "        phi_keys = [('gene', f'phi{i}', 'gene') for i in [1,2,3,4]]\n",
    "        phi_keys = [k for k in phi_keys if (k in edge_index_dict and edge_index_dict[k].numel() > 0)]\n",
    "        if not phi_keys:\n",
    "            raise RuntimeError(\"No Ï†-edges in edge_index_dict\")\n",
    "        # Enforce that Ï†4 really exists\n",
    "        if ('gene','phi4','gene') not in edge_index_dict or edge_index_dict[('gene','phi4','gene')].numel() == 0:\n",
    "            raise RuntimeError(\"Ï†4 required but missing in edge_index_dict during forward.\")\n",
    "\n",
    "        per_phi = []\n",
    "        for k in phi_keys:\n",
    "            phi = k[1]\n",
    "            e = edge_index_dict[k]\n",
    "            h1 = self.gat1[phi](xg, e)\n",
    "            h1 = self.norm1(F.relu(h1) + xg)\n",
    "            h1 = self.dropout(h1)\n",
    "            h2 = self.gat2[phi](h1, e)\n",
    "            h2 = self.norm2(F.relu(h2) + h1)\n",
    "            per_phi.append(h2)\n",
    "\n",
    "        H = torch.stack(per_phi, dim=1)\n",
    "        fused, alpha = self.semantic_att(H)\n",
    "\n",
    "        if self.use_post_mha:\n",
    "            hv_key = ('gene','hv','virus') if ('gene','hv','virus') in edge_index_dict else ('gene','interacts','virus')\n",
    "            if hv_key in edge_index_dict and edge_index_dict[hv_key].numel() > 0:\n",
    "                zv = aggregate_virus_mean(xv, edge_index_dict[hv_key], xg.size(0))\n",
    "            else:\n",
    "                zv = xv.new_zeros(xg.size())\n",
    "            tokens = torch.stack([xg, fused, zv], dim=1)\n",
    "            fused = self.post_mha(tokens)\n",
    "\n",
    "        logit = self.cls(fused).squeeze(-1)\n",
    "        return {'gene': logit}\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Eval Utils\n",
    "# -----------------------------\n",
    "def scale_features_in_fold(data: HeteroData, train_idx: np.ndarray):\n",
    "    Xg = data['gene'].x.detach().cpu().numpy()\n",
    "    sc_gene = StandardScaler().fit(Xg[train_idx])\n",
    "    data['gene'].x = torch.tensor(sc_gene.transform(Xg), dtype=torch.float, device=data['gene'].x.device)\n",
    "\n",
    "    Xv = data['virus'].x.detach().cpu().numpy()\n",
    "    sc_v = StandardScaler().fit(Xv)\n",
    "    data['virus'].x = torch.tensor(sc_v.transform(Xv), dtype=torch.float, device=data['virus'].x.device)\n",
    "    return data, sc_gene, sc_v\n",
    "\n",
    "def apply_scalers_to_data(sc_gene: StandardScaler, sc_virus: StandardScaler, data: HeteroData):\n",
    "    data['gene'].x  = torch.tensor(sc_gene.transform(data['gene'].x.detach().cpu().numpy()),\n",
    "                                   dtype=torch.float, device=data['gene'].x.device)\n",
    "    data['virus'].x = torch.tensor(sc_virus.transform(data['virus'].x.detach().cpu().numpy()),\n",
    "                                   dtype=torch.float, device=data['virus'].x.device)\n",
    "    return data\n",
    "\n",
    "def build_train_edges_with_dropedge_phi(data: HeteroData, p_phi=DROPEDGE_PHI_P) -> Dict[Tup[str,str,str], torch.Tensor]:\n",
    "    cur = dict(data.edge_index_dict)\n",
    "    Ng = data['gene'].x.size(0)\n",
    "    for name in ['phi1','phi2','phi3','phi4']:\n",
    "        k = ('gene', name, 'gene')\n",
    "        if k in cur and cur[k].numel() > 0 and p_phi > 0:\n",
    "            e = cur[k]\n",
    "            e, _ = dropout_adj(e, p=p_phi, force_undirected=True, num_nodes=Ng)\n",
    "            cur[k] = e\n",
    "    return cur\n",
    "\n",
    "# ---- Youdenâ€™s J = TPR + TNR âˆ’ 1 ----\n",
    "def find_best_threshold(y_true: np.ndarray, y_prob: np.ndarray, grid: int = 201, metric: str = 'f1') -> float:\n",
    "    thrs = np.linspace(0.0, 1.0, grid)\n",
    "    best_t, best_s = 0.5, -1.0\n",
    "    y_true = y_true.astype(int)\n",
    "\n",
    "    for t in thrs:\n",
    "        pred = (y_prob >= t).astype(int)\n",
    "        if metric == 'f1':\n",
    "            s = f1_score(y_true, pred, zero_division=0)\n",
    "        elif metric == 'youden':\n",
    "            tp = np.sum((y_true == 1) & (pred == 1))\n",
    "            tn = np.sum((y_true == 0) & (pred == 0))\n",
    "            fp = np.sum((y_true == 0) & (pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (pred == 0))\n",
    "            tpr = tp / (tp + fn + 1e-12)\n",
    "            tnr = tn / (tn + fp + 1e-12)\n",
    "            s = tpr + tnr - 1.0\n",
    "        else:\n",
    "            s = f1_score(y_true, pred, zero_division=0)\n",
    "        if s > best_s:\n",
    "            best_s, best_t = s, t\n",
    "    return float(best_t)\n",
    "\n",
    "# [INDUCTIVE] train on train-graph, validate on trainâˆªvalid-graph\n",
    "def train_one_fold_inductive(model: SimpleHAN,\n",
    "                             data_train_graph: HeteroData,\n",
    "                             data_val_graph: HeteroData,\n",
    "                             train_idx, val_idx,\n",
    "                             lr=LR, max_epochs=MAX_EPOCHS, weight_decay=WEIGHT_DECAY, patience=PATIENCE):\n",
    "    model = model.to(DEVICE)\n",
    "    data_train_graph = data_train_graph.to(DEVICE)\n",
    "    data_val_graph   = data_val_graph.to(DEVICE)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_idx_t = torch.as_tensor(train_idx, dtype=torch.long, device=DEVICE)\n",
    "    val_idx_t   = torch.as_tensor(val_idx,   dtype=torch.long, device=DEVICE)\n",
    "    targets = data_train_graph['gene'].y.float()  # same y across graphs\n",
    "\n",
    "    best_state = None\n",
    "    best_metric  = -1.0\n",
    "    bad = 0\n",
    "\n",
    "    for _ in range(max_epochs):\n",
    "        model.train()\n",
    "        cur_edges = build_train_edges_with_dropedge_phi(data_train_graph)\n",
    "        logits = model(data_train_graph.x_dict, cur_edges)['gene']\n",
    "        loss = criterion(logits[train_idx_t], targets[train_idx_t])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # Ø§Ø®ØªÛŒØ§Ø±ÛŒ\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation on trainâˆªvalid graph (no DropEdge)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(data_val_graph.x_dict, data_val_graph.edge_index_dict)['gene'][val_idx_t]\n",
    "            val_prob   = torch.sigmoid(val_logits).detach().cpu().numpy()\n",
    "            val_true   = targets[val_idx_t].detach().cpu().numpy()\n",
    "            val_auc    = roc_auc_score(val_true, val_prob)\n",
    "\n",
    "        if val_auc > best_metric:\n",
    "            best_metric = val_auc\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def evaluate_split(model: SimpleHAN, data: HeteroData, idx, thr: Optional[float] = None):\n",
    "    model.eval()\n",
    "    data = data.to(DEVICE)\n",
    "    idx_t = torch.as_tensor(idx, dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x_dict, data.edge_index_dict)['gene'][idx_t]\n",
    "        prob   = torch.sigmoid(logits).cpu().numpy()\n",
    "        true   = data['gene'].y[idx_t].cpu().numpy()\n",
    "    use_thr = 0.5 if thr is None else float(thr)\n",
    "    pred = (prob >= use_thr).astype(int)\n",
    "    brier = float(np.mean((prob - true)**2))\n",
    "    res = dict(\n",
    "        precision = precision_score(true, pred, zero_division=0),\n",
    "        recall    = recall_score(true, pred, zero_division=0),\n",
    "        f1        = f1_score(true, pred, zero_division=0),\n",
    "        accuracy  = accuracy_score(true, pred),\n",
    "        auc_roc   = roc_auc_score(true, prob),\n",
    "        auc_pr    = average_precision_score(true, prob),\n",
    "        brier     = brier,\n",
    "        threshold = use_thr\n",
    "    )\n",
    "    return res, prob, pred\n",
    "\n",
    "# -----------------------------\n",
    "# (NEW) Search space & inner-CV tuning (Nested CV) â€” returns stats for table\n",
    "# -----------------------------\n",
    "def get_search_space():\n",
    "    \"\"\"\n",
    "    Minimal, fast search space for nested CV (expand if you have budget).\n",
    "    \"\"\"\n",
    "    space = []\n",
    "    for h in [128, 256]:\n",
    "        for heads in [2, 4]:\n",
    "            for dr in [0.2, 0.3]:\n",
    "                for lr in [5e-4, 1e-3]:\n",
    "                    for wd in [1e-5, 1e-4]:\n",
    "                        space.append(dict(HIDDEN=h, HEADS=heads, DROPOUT=dr, LR=lr, WEIGHT_DECAY=wd))\n",
    "    return space\n",
    "\n",
    "def inner_cv_tune(data_base: HeteroData,\n",
    "                  y: np.ndarray,\n",
    "                  outer_train_idx: np.ndarray,\n",
    "                  in_gene: int,\n",
    "                  in_virus: int):\n",
    "    \"\"\"\n",
    "    Nested CV: choose best hyperparameters using ONLY outer-train.\n",
    "    Returns:\n",
    "      best_cfg, inner_stats (dict with mean/sd and per-fold lists for AUROC/AUPRC)\n",
    "    \"\"\"\n",
    "    inner_skf = StratifiedKFold(n_splits=K_INNER, shuffle=True, random_state=SEED)\n",
    "    search_space = get_search_space()\n",
    "\n",
    "    best_cfg = None\n",
    "    best_auc = -1.0\n",
    "    best_aupr = -1.0\n",
    "    best_lists = None  # (aucs, auprs)\n",
    "\n",
    "    for cfg in search_space:\n",
    "        aucs, auprs = [], []\n",
    "\n",
    "        for inner_train_rel, inner_val_rel in inner_skf.split(np.zeros_like(y[outer_train_idx]), y[outer_train_idx]):\n",
    "            inner_train_idx = outer_train_idx[inner_train_rel]\n",
    "            inner_val_idx   = outer_train_idx[inner_val_rel]\n",
    "\n",
    "            data_inner_train    = induce_graph_by_genes(data_base, inner_train_idx)\n",
    "            data_inner_trainval = induce_graph_by_genes(data_base, np.concatenate([inner_train_idx, inner_val_idx]))\n",
    "\n",
    "            data_inner_train, sc_gene, sc_v = scale_features_in_fold(copy.deepcopy(data_inner_train), inner_train_idx)\n",
    "            data_inner_trainval = apply_scalers_to_data(sc_gene, sc_v, data_inner_trainval)\n",
    "\n",
    "            phi_names_train = [k[1] for k in data_inner_train.edge_index_dict.keys()\n",
    "                               if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_inner_train[k].edge_index.numel()>0)]\n",
    "            if 'phi4' not in phi_names_train:\n",
    "                continue\n",
    "\n",
    "            model = SimpleHAN(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                              phi_names=phi_names_train,\n",
    "                              hidden=cfg['HIDDEN'], heads=cfg['HEADS'], dropout=cfg['DROPOUT'],\n",
    "                              use_post_mha=USE_POST_MHA, use_virus_token=True)\n",
    "\n",
    "            model = train_one_fold_inductive(model,\n",
    "                                             data_inner_train,\n",
    "                                             data_inner_trainval,\n",
    "                                             inner_train_idx,\n",
    "                                             inner_val_idx,\n",
    "                                             lr=cfg['LR'],\n",
    "                                             max_epochs=MAX_EPOCHS,\n",
    "                                             weight_decay=cfg['WEIGHT_DECAY'],\n",
    "                                             patience=PATIENCE)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(data_inner_trainval.x_dict, data_inner_trainval.edge_index_dict)['gene'][torch.as_tensor(inner_val_idx, device=DEVICE)]\n",
    "                val_prob   = torch.sigmoid(val_logits).cpu().numpy()\n",
    "                val_true   = y[inner_val_idx]\n",
    "            auc  = roc_auc_score(val_true, val_prob)\n",
    "            aupr = average_precision_score(val_true, val_prob)\n",
    "            aucs.append(auc); auprs.append(aupr)\n",
    "\n",
    "        if len(aucs) == 0:\n",
    "            continue\n",
    "\n",
    "        mean_auc  = float(np.mean(aucs))\n",
    "        mean_aupr = float(np.mean(auprs))\n",
    "\n",
    "        if (mean_auc > best_auc) or (np.isclose(mean_auc, best_auc) and mean_aupr > best_aupr):\n",
    "            best_auc, best_aupr = mean_auc, mean_aupr\n",
    "            best_cfg = cfg\n",
    "            best_lists = (aucs, auprs)\n",
    "\n",
    "    if best_cfg is None:\n",
    "        best_cfg = dict(HIDDEN=256, HEADS=4, DROPOUT=0.3, LR=1e-3, WEIGHT_DECAY=1e-4)\n",
    "        inner_stats = dict(auc_mean=np.nan, auc_sd=np.nan, aupr_mean=np.nan, aupr_sd=np.nan,\n",
    "                           auc_list=[], aupr_list=[])\n",
    "        return best_cfg, inner_stats\n",
    "\n",
    "    aucs, auprs = best_lists\n",
    "    inner_stats = dict(\n",
    "        auc_mean=float(np.mean(aucs)), auc_sd=float(np.std(aucs)),\n",
    "        aupr_mean=float(np.mean(auprs)), aupr_sd=float(np.std(auprs)),\n",
    "        auc_list=aucs, aupr_list=auprs\n",
    "    )\n",
    "    return best_cfg, inner_stats\n",
    "\n",
    "# -----------------------------\n",
    "# Run: Stratified KFold CV (INDUCTIVE) â€” NESTED TUNING + CSV\n",
    "# -----------------------------\n",
    "def run_kfold_han_style():\n",
    "    # base (no Ï†)\n",
    "    data_base, genes_df, virus_df = build_base_heterodata(apply_undirected=True)\n",
    "    # full graph with Ï† (for final outer test)\n",
    "    data_full = copy.deepcopy(data_base)\n",
    "    data_full = add_budget_relations(data_full)\n",
    "    data_full = add_metapath_phi_edges(data_full, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "\n",
    "    in_gene  = data_base['gene'].x.size(1)\n",
    "    in_virus = data_base['virus'].x.size(1)\n",
    "    y = data_base['gene'].y.cpu().numpy()\n",
    "\n",
    "    # sanity: Ï†4 exists on full graph\n",
    "    phi_names_full = [k[1] for k in data_full.edge_index_dict.keys()\n",
    "                      if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_full[k].edge_index.numel()>0)]\n",
    "    if 'phi4' not in phi_names_full:\n",
    "        raise RuntimeError(\"Ï†4 is required but missing in constructed meta-paths on full graph.\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    metrics_list_best = []\n",
    "    metrics_list_05   = []\n",
    "    tuning_rows = []   # per-outer-fold tuning summaries\n",
    "\n",
    "    for fold, (outer_train_idx, outer_test_idx) in enumerate(skf.split(np.zeros_like(y), y), 1):\n",
    "        print(f\"\\n====[Outer Fold {fold}]====\")\n",
    "        print(f\"train dist -> {dict(zip(*np.unique(y[outer_train_idx], return_counts=True)))}\")\n",
    "        print(f\"test  dist -> {dict(zip(*np.unique(y[outer_test_idx],  return_counts=True)))}\")\n",
    "\n",
    "        # (A) tune hyperparameters via inner-CV on outer-train only\n",
    "        best_cfg, inner_stats = inner_cv_tune(data_base, y, outer_train_idx, in_gene, in_virus)\n",
    "        print(f\"[Outer {fold}] best cfg: {best_cfg} | inner AUROC={inner_stats['auc_mean']:.3f}Â±{inner_stats['auc_sd']:.3f}, AUPRC={inner_stats['aupr_mean']:.3f}Â±{inner_stats['aupr_sd']:.3f}\")\n",
    "\n",
    "        # (B) build outer-valid (20% of outer-train) â€” for threshold selection & early stop target\n",
    "        inner_train_idx, outer_valid_idx = train_test_split(\n",
    "            outer_train_idx, test_size=0.2, random_state=SEED, stratify=y[outer_train_idx]\n",
    "        )\n",
    "\n",
    "        # induce graphs for outer-train and (trainâˆªvalid)\n",
    "        data_train    = induce_graph_by_genes(data_base, inner_train_idx)\n",
    "        data_trainval = induce_graph_by_genes(data_base, np.concatenate([inner_train_idx, outer_valid_idx]))\n",
    "\n",
    "        # scale from inner_train only; apply to trainval and full\n",
    "        data_train, sc_gene, sc_v = scale_features_in_fold(copy.deepcopy(data_train), inner_train_idx)\n",
    "        data_trainval    = apply_scalers_to_data(sc_gene, sc_v, data_trainval)\n",
    "        data_full_scaled = apply_scalers_to_data(sc_gene, sc_v, copy.deepcopy(data_full))\n",
    "\n",
    "        # ensure Ï†4 exists on train graph\n",
    "        phi_names_train = [k[1] for k in data_train.edge_index_dict.keys()\n",
    "                           if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_train[k].edge_index.numel()>0)]\n",
    "        if 'phi4' not in phi_names_train:\n",
    "            raise RuntimeError(\"Ï†4 is required but missing in TRAIN meta-paths (inductive).\")\n",
    "\n",
    "        # (C) train final model on outer-train using best cfg\n",
    "        model = SimpleHAN(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                          phi_names=phi_names_train,\n",
    "                          hidden=best_cfg['HIDDEN'], heads=best_cfg['HEADS'], dropout=best_cfg['DROPOUT'],\n",
    "                          use_post_mha=USE_POST_MHA, use_virus_token=True)\n",
    "\n",
    "        model = train_one_fold_inductive(model,\n",
    "                                         data_train,\n",
    "                                         data_trainval,\n",
    "                                         inner_train_idx,\n",
    "                                         outer_valid_idx,\n",
    "                                         lr=best_cfg['LR'],\n",
    "                                         max_epochs=MAX_EPOCHS,\n",
    "                                         weight_decay=best_cfg['WEIGHT_DECAY'],\n",
    "                                         patience=PATIENCE)\n",
    "\n",
    "        # (D) choose threshold on outer-valid\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(data_trainval.x_dict, data_trainval.edge_index_dict)['gene'][torch.as_tensor(outer_valid_idx, device=DEVICE)]\n",
    "            val_prob   = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            val_true   = y[outer_valid_idx]\n",
    "        best_thr = find_best_threshold(val_true, val_prob, grid=201, metric='f1')\n",
    "\n",
    "        # (E) evaluate on outer-test (independent)\n",
    "        test_res_best, _, _ = evaluate_split(model, data_full_scaled, outer_test_idx, thr=best_thr)\n",
    "        test_res_05,   _, _ = evaluate_split(model, data_full_scaled, outer_test_idx, thr=0.5)\n",
    "\n",
    "        print(f\"[Outer {fold}] (best-F1 Thr={test_res_best['threshold']:.2f}) \"\n",
    "              f\"AUC={test_res_best['auc_roc']:.3f}  AUPR={test_res_best['auc_pr']:.3f}  \"\n",
    "              f\"F1={test_res_best['f1']:.3f}  Acc={test_res_best['accuracy']:.3f}  \"\n",
    "              f\"Prec={test_res_best['precision']:.3f}  Rec={test_res_best['recall']:.3f}  \"\n",
    "              f\"Brier={test_res_best['brier']:.4f}\")\n",
    "\n",
    "        print(f\"[Outer {fold}] (thr=0.50) \"\n",
    "              f\"AUC={test_res_05['auc_roc']:.3f}  AUPR={test_res_05['auc_pr']:.3f}  \"\n",
    "              f\"F1={test_res_05['f1']:.3f}  Acc={test_res_05['accuracy']:.3f}  \"\n",
    "              f\"Prec={test_res_05['precision']:.3f}  Rec={test_res_05['recall']:.3f}  \"\n",
    "              f\"Brier={test_res_05['brier']:.4f}\")\n",
    "\n",
    "        metrics_list_best.append(test_res_best)\n",
    "        metrics_list_05.append(test_res_05)\n",
    "\n",
    "        # Ø¬Ù…Ø¹ Ø¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÛŒÙˆÙ†ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„\n",
    "        tuning_rows.append(dict(\n",
    "            virus = VIRUS_NAME,\n",
    "            outer_fold = fold,\n",
    "            HIDDEN = best_cfg['HIDDEN'],\n",
    "            HEADS  = best_cfg['HEADS'],\n",
    "            DROPOUT = best_cfg['DROPOUT'],\n",
    "            LR = best_cfg['LR'],\n",
    "            WEIGHT_DECAY = best_cfg['WEIGHT_DECAY'],\n",
    "            Post_MHA = int(USE_POST_MHA),\n",
    "            K_VH = K_VH_VIRUS_TO_H,\n",
    "            K_VV = K_VV_PER_VIRUS,\n",
    "            K_HH_phi2 = K_HH_PHI2,\n",
    "            K_HH_phi3 = K_HH_PHI3,\n",
    "            inner_auc_mean = inner_stats['auc_mean'],\n",
    "            inner_auc_sd   = inner_stats['auc_sd'],\n",
    "            inner_aupr_mean= inner_stats['aupr_mean'],\n",
    "            inner_aupr_sd  = inner_stats['aupr_sd'],\n",
    "            outer_test_auc = test_res_best['auc_roc'],\n",
    "            outer_test_aupr= test_res_best['auc_pr'],\n",
    "            outer_test_f1  = test_res_best['f1']\n",
    "        ))\n",
    "\n",
    "    keys = ['precision','recall','f1','accuracy','auc_roc','auc_pr','brier']\n",
    "    print(\"\\n=== Stratified KFold (Test @ best-F1 threshold from outer-valid) ===\")\n",
    "    for k in keys:\n",
    "        m = float(np.mean([d[k] for d in metrics_list_best]))\n",
    "        s = float(np.std([d[k] for d in metrics_list_best]))\n",
    "        print(f\"{k:>10s}: {m:.4f} Â± {s:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Stratified KFold (Test @ fixed threshold = 0.50) ===\")\n",
    "    for k in keys:\n",
    "        m = float(np.mean([d[k] for d in metrics_list_05]))\n",
    "        s = float(np.std([d[k] for d in metrics_list_05]))\n",
    "        print(f\"{k:>10s}: {m:.4f} Â± {s:.4f}\")\n",
    "\n",
    "    # Ù†ÙˆØ´ØªÙ† CSV ØªÛŒÙˆÙ†ÛŒÙ†Ú¯ + Ø±Ø¯ÛŒÙ Ø§Ø¬Ù…Ø§Ø¹ÛŒ\n",
    "    df_tune = pd.DataFrame(tuning_rows)\n",
    "    df_tune.to_csv(TUNING_SUMMARY_PATH, index=False)\n",
    "    print(f\"\\nSaved per-fold tuning summary -> {TUNING_SUMMARY_PATH}\")\n",
    "\n",
    "    if not df_tune.empty:\n",
    "        cfg_cols = ['HIDDEN','HEADS','DROPOUT','LR','WEIGHT_DECAY','Post_MHA','K_VH','K_VV','K_HH_phi2','K_HH_phi3']\n",
    "        consensus = df_tune[cfg_cols].mode().iloc[0].to_dict()\n",
    "\n",
    "        cons_inner_auc_mean = df_tune['inner_auc_mean'].mean()\n",
    "        cons_inner_auc_sd   = df_tune['inner_auc_mean'].std()\n",
    "        cons_inner_aupr_mean= df_tune['inner_aupr_mean'].mean()\n",
    "        cons_inner_aupr_sd  = df_tune['inner_aupr_mean'].std()\n",
    "\n",
    "        cons_row = {\n",
    "            'virus': VIRUS_NAME, **consensus,\n",
    "            'inner_AUROC_meanÂ±SD': f\"{cons_inner_auc_mean:.3f} Â± {cons_inner_auc_sd:.3f}\",\n",
    "            'inner_AUPRC_meanÂ±SD': f\"{cons_inner_aupr_mean:.3f} Â± {cons_inner_aupr_sd:.3f}\"\n",
    "        }\n",
    "        print(\"\\nConsensus (table-ready):\")\n",
    "        print(cons_row)\n",
    "        pd.DataFrame([cons_row]).to_csv(f\"consensus_hparams_{VIRUS_NAME}.csv\", index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# (Optional) Final training + Predict all genes\n",
    "# -----------------------------\n",
    "def run_fit_and_predict_all():\n",
    "    # base + full\n",
    "    data_base, genes_df, virus_df = build_base_heterodata(apply_undirected=True)\n",
    "    data_full = copy.deepcopy(data_base)\n",
    "    data_full = add_budget_relations(data_full)\n",
    "    data_full = add_metapath_phi_edges(data_full, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "\n",
    "    in_gene  = data_base['gene'].x.size(1)\n",
    "    in_virus = data_base['virus'].x.size(1)\n",
    "    y        = data_base['gene'].y.cpu().numpy()\n",
    "    all_idx  = np.arange(len(y))\n",
    "\n",
    "    phi_names = [k[1] for k in data_full.edge_index_dict.keys()\n",
    "                 if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_full[k].edge_index.numel()>0)]\n",
    "    if 'phi4' not in phi_names:\n",
    "        raise RuntimeError(\"Ï†4 is required but missing in constructed meta-paths (final inference).\")\n",
    "\n",
    "    tr_full, val_hold = train_test_split(all_idx, test_size=VAL_SIZE, random_state=SEED, stratify=y)\n",
    "\n",
    "    # [INDUCTIVE] graphs for training and validation\n",
    "    data_train_full = induce_graph_by_genes(data_base, tr_full)\n",
    "    data_trainval   = induce_graph_by_genes(data_base, np.concatenate([tr_full, val_hold]))\n",
    "\n",
    "    # scale from train only, apply to both graphs and to full\n",
    "    data_train_full, sc_gene_full, sc_virus_full = scale_features_in_fold(copy.deepcopy(data_train_full), tr_full)\n",
    "    data_trainval = apply_scalers_to_data(sc_gene_full, sc_virus_full, data_trainval)\n",
    "    data_full     = apply_scalers_to_data(sc_gene_full, sc_virus_full, data_full)\n",
    "\n",
    "    model_full = SimpleHAN(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                           phi_names=[k[1] for k in data_train_full.edge_index_dict.keys() if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_train_full[k].edge_index.numel()>0)],\n",
    "                           hidden=HIDDEN, heads=HEADS, dropout=DROPOUT,\n",
    "                           use_post_mha=USE_POST_MHA, use_virus_token=True).to(DEVICE)\n",
    "\n",
    "    model_full = train_one_fold_inductive(model_full, data_train_full, data_trainval,\n",
    "                                          tr_full, val_hold,\n",
    "                                          lr=LR, max_epochs=MAX_EPOCHS, weight_decay=WEIGHT_DECAY,\n",
    "                                          patience=PATIENCE)\n",
    "\n",
    "    # choose threshold on val (inductive val graph)\n",
    "    model_full.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model_full(data_trainval.x_dict, data_trainval.edge_index_dict)['gene'][torch.as_tensor(val_hold, device=DEVICE)]\n",
    "        val_prob   = torch.sigmoid(val_logits).cpu().numpy()\n",
    "        val_true   = y[val_hold]\n",
    "    best_thr = find_best_threshold(val_true, val_prob, grid=201, metric='f1')\n",
    "    print(f\"[Final Fit] chosen threshold (best F1 on held-out val): {best_thr:.3f}\")\n",
    "\n",
    "    # ----- Predict ALL genes on full graph -----\n",
    "    df_all = pd.read_csv(FILE_GENE_ALL)\n",
    "    interactions_df2 = pd.read_excel(FILE_GV)\n",
    "    biogrid_df2      = pd.read_csv(FILE_PPI_H)\n",
    "    virus_df2        = pd.read_excel(FILE_VIRUS)\n",
    "    vv_df2           = pd.read_excel(FILE_PPI_VV)\n",
    "\n",
    "    genes_to_index_all = {g: i for i, g in enumerate(df_all['Genes'])}\n",
    "    virus_to_index2    = {v: i for i, v in enumerate(virus_df2['GenesSymbolVirus'])}\n",
    "\n",
    "    interactions_df2 = interactions_df2[\n",
    "        (interactions_df2['Genes'].isin(df_all['Genes'])) &\n",
    "        (interactions_df2['GenesVirus'].isin(virus_df2['GenesSymbolVirus']))\n",
    "    ]\n",
    "    biogrid_df2 = biogrid_df2[\n",
    "        (biogrid_df2['Ensembl_ID_A'].isin(df_all['Genes'])) &\n",
    "        (biogrid_df2['Ensembl_ID_B'].isin(df_all['Genes']))\n",
    "    ]\n",
    "\n",
    "    X_gene_all  = torch.tensor(df_all.iloc[:, 1:].values, dtype=torch.float)\n",
    "    X_virus_all = torch.tensor(virus_df2.iloc[:, 1:].values, dtype=torch.float)\n",
    "\n",
    "    gv2 = torch.tensor([\n",
    "        [genes_to_index_all[row['Genes']], virus_to_index2[row['GenesVirus']]]\n",
    "        for _, row in interactions_df2.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    hh2 = torch.tensor([\n",
    "        [genes_to_index_all[row['Ensembl_ID_A']], genes_to_index_all[row['Ensembl_ID_B']]]\n",
    "        for _, row in biogrid_df2.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    a_col = 'Official_Symbol_A_Zika'; b_col = 'Official_Symbol_B_Zika'\n",
    "    if a_col in vv_df2.columns and b_col in vv_df2.columns:\n",
    "        vv2 = torch.tensor([\n",
    "            [virus_to_index2.get(row[a_col], -1), virus_to_index2.get(row[b_col], -1)]\n",
    "            for _, row in vv_df2.iterrows()\n",
    "        ], dtype=torch.long).t().contiguous()\n",
    "        if vv2.numel() > 0:\n",
    "            mask_valid = (vv2 >= 0).all(dim=0)\n",
    "            vv2 = vv2[:, mask_valid]\n",
    "    else:\n",
    "        vv2 = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    data_all = HeteroData()\n",
    "    data_all['gene'].x  = X_gene_all\n",
    "    data_all['virus'].x = X_virus_all\n",
    "    data_all['gene','interacts','virus'].edge_index = gv2\n",
    "    data_all['gene','interacts','gene' ].edge_index = hh2\n",
    "    if vv2.numel() > 0:\n",
    "        data_all['virus','interacts','virus'].edge_index = vv2\n",
    "    data_all = T.ToUndirected()(data_all)\n",
    "\n",
    "    data_all = add_budget_relations(data_all)\n",
    "    data_all = add_metapath_phi_edges(data_all, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "\n",
    "    # apply scalers from inductive training\n",
    "    data_all = apply_scalers_to_data(sc_gene_full, sc_virus_full, data_all)\n",
    "\n",
    "    model_full.eval()\n",
    "    data_all = data_all.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits_all = model_full(data_all.x_dict, data_all.edge_index_dict)['gene']\n",
    "        probs_all  = torch.sigmoid(logits_all).cpu().numpy()\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        'Gene_Name': df_all['Genes'],\n",
    "        'Predicted_Probability': probs_all,\n",
    "        'Predicted_Label_bestF1': (probs_all >= best_thr).astype(int),\n",
    "        'Predicted_Label_thr0.5': (probs_all >= 0.5).astype(int)\n",
    "    })\n",
    "    out_path = 'Wholedata-HAN-predictions-Zika-final.csv'\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved predictions -> {out_path}\")\n",
    "    print(f\"Used thresholds: best-F1={best_thr:.3f} and fixed=0.50\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Cross-validated evaluation (INDUCTIVE, Nested Tuning) + CSV summary\n",
    "    run_kfold_han_style()\n",
    "\n",
    "    # 2) (Optional) Final fit + predict-all (INDUCTIVE training, full-graph inference)\n",
    "    run_fit_and_predict_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Ablation Experiments\n",
    "# =========================\n",
    "# ===== Ablation Suite ====\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- 1) Ù†Ú¯Ø§Ø´Øª Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø± Ø¨Ù‡ regex (Ù¾ÛŒØ´ÙˆÙ†Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ø®ÙˆØ¯Øª ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø¯Ù‡)\n",
    "FEATURE_GROUP_PATTERNS = {\n",
    "    # 1) Sequence descriptors (DNA/Protein)\n",
    "    \"sequence\": r\"^(seq\\.|aa_|dna_|kmer_|pseaa_|psedna_|conjoint_|qsorder_)\",\n",
    "    # 2) GO/Pathway enrichment + KEGG/Reactome counts\n",
    "    \"go_pathway\": r\"^(go_|kegg_|reactome_)\",\n",
    "    # 3) Protein domains & motifs & UTRs/PTMs\n",
    "    \"domains_motifs\": r\"^(pfam_|domain_|coil_|tmhmm_|ptm_|signalpeptide_|utr_)\",\n",
    "    # 4) Conservation / Homology / Orthologs\n",
    "    \"conservation\": r\"^(psiblast_|homolog_|ortholog_|ka_ks_|evalue_)\",\n",
    "    # 5) PPI topology & roles\n",
    "    \"ppi_topology\": r\"^(degree_|betweenness_|closeness_|pagerank_|refex_|rolx_|egonet_)\",\n",
    "    # 6) Embeddings + Localization\n",
    "    \"emb_loc\": r\"^(n2v_|node2vec_|deeploc_)\"\n",
    "}\n",
    "\n",
    "def drop_feature_groups(features_df: pd.DataFrame, groups_to_drop: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ø­Ø°Ù Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø± Ø§Ø² DataFrame (Ø³ØªÙˆÙ† Class Ùˆ Genes Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯).\n",
    "    Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ FEATURE_GROUP_PATTERNS Ø¨Ø¯Ù‡.\n",
    "    \"\"\"\n",
    "    keep_cols = ['Genes', 'Class']\n",
    "    patterns = [re.compile(FEATURE_GROUP_PATTERNS[g]) for g in groups_to_drop]\n",
    "    drop_cols = []\n",
    "    for c in features_df.columns:\n",
    "        if c in keep_cols:\n",
    "            continue\n",
    "        if any(p.match(c) for p in patterns):\n",
    "            drop_cols.append(c)\n",
    "    out = features_df.drop(columns=drop_cols, errors='ignore')\n",
    "    print(f\"[Ablation] Dropped feature groups={groups_to_drop} -> removed {len(drop_cols)} columns; kept {out.shape[1]-2} features.\")\n",
    "    return out\n",
    "\n",
    "# --- 2) Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨ ÛŒØ§Ù„â€ŒÙ‡Ø§ Ùˆ Ù…ØªØ§Ù¾Ø«â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø²\n",
    "def add_budget_relations_with_toggles(data: HeteroData,\n",
    "                                      use_hh=True, use_hv=True, use_vv=True) -> HeteroData:\n",
    "    Ng = data['gene'].x.size(0)\n",
    "    Nv = data['virus'].x.size(0)\n",
    "\n",
    "    if use_hh and ('gene','interacts','gene') in data.edge_index_dict:\n",
    "        hh_e = data[('gene','interacts','gene')].edge_index\n",
    "        data['gene','hh2','gene'].edge_index = topk_per_src(hh_e, Ng, K_HH_PHI2, seed=SEED)\n",
    "        data['gene','hh3','gene'].edge_index = topk_per_src(hh_e, Ng, K_HH_PHI3, seed=SEED+1)\n",
    "\n",
    "    if use_hv and ('gene','interacts','virus') in data.edge_index_dict:\n",
    "        hv_e = data[('gene','interacts','virus')].edge_index\n",
    "        data['gene','hv','virus'].edge_index = topk_per_src(hv_e, Ng, K_HV_GENE_TO_V, seed=SEED+2)\n",
    "\n",
    "    if use_hv and ('virus','rev_interacts','gene') in data.edge_index_dict:\n",
    "        vh_e = data[('virus','rev_interacts','gene')].edge_index\n",
    "        data['virus','vh','gene'].edge_index = topk_per_src(vh_e, Nv, K_VH_VIRUS_TO_H, seed=SEED+3)\n",
    "\n",
    "    if use_vv and ('virus','interacts','virus') in data.edge_index_dict:\n",
    "        vv_e = data[('virus','interacts','virus')].edge_index\n",
    "        data['virus','vv','virus'].edge_index = topk_per_src(vv_e, Nv, K_VV_PER_VIRUS, seed=SEED+4)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_metapath_phi_edges_allowed(data: HeteroData,\n",
    "                                   allow_phi: List[int],\n",
    "                                   enforce_phi4: bool) -> HeteroData:\n",
    "    Ng = data['gene'].x.size(0); Nv = data['virus'].x.size(0)\n",
    "    device = data['gene'].x.device\n",
    "\n",
    "    def get_e(key):\n",
    "        return data[key].edge_index if key in data.edge_index_dict else torch.empty((2,0), dtype=torch.long, device=device)\n",
    "\n",
    "    hv  = get_e(('gene','hv','virus'))\n",
    "    vh  = get_e(('virus','vh','gene')) if ('virus','vh','gene') in data.edge_index_dict else get_e(('virus','interacts','gene'))\n",
    "    hh2 = get_e(('gene','hh2','gene'))\n",
    "    hh3 = get_e(('gene','hh3','gene'))\n",
    "    vv  = get_e(('virus','vv','virus'))\n",
    "\n",
    "    if 2 in allow_phi and hh2.numel() > 0:\n",
    "        data['gene','phi2','gene'].edge_index = unique_edges(hh2)\n",
    "\n",
    "    if 1 in allow_phi and hv.numel() > 0 and vh.numel() > 0:\n",
    "        phi1 = compose_edges(hv, vh, Ng, Nv)\n",
    "        if phi1.numel() > 0:\n",
    "            data['gene','phi1','gene'].edge_index = phi1\n",
    "\n",
    "    if 3 in allow_phi and hh3.numel() > 0 and hv.numel() > 0 and vh.numel() > 0:\n",
    "        tmp = compose_edges(hh3, hv, Ng, Ng)\n",
    "        phi3 = compose_edges(tmp, vh, Ng, Nv)\n",
    "        if phi3.numel() > 0:\n",
    "            data['gene','phi3','gene'].edge_index = phi3\n",
    "\n",
    "    if 4 in allow_phi:\n",
    "        if vv.numel() > 0 and hv.numel() > 0 and vh.numel() > 0:\n",
    "            tmp = compose_edges(hv, vv, Ng, Nv)\n",
    "            phi4 = compose_edges(tmp, vh, Ng, Nv)\n",
    "            if phi4.numel() > 0:\n",
    "                data['gene','phi4','gene'].edge_index = phi4\n",
    "        elif enforce_phi4:\n",
    "            raise RuntimeError(\"Ï†4 required but insufficient VV/HV/VH to compose.\")\n",
    "\n",
    "    # fallback Ø§Ú¯Ø± Ù‡ÛŒÚ† Ï† Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø´Ø¯:\n",
    "    has_any_phi = any(k[1].startswith('phi') for k in data.edge_index_dict.keys() if k[0]=='gene' and k[2]=='gene')\n",
    "    if not has_any_phi:\n",
    "        if ('gene','interacts','gene') in data.edge_index_dict:\n",
    "            data['gene','phi2','gene'].edge_index = unique_edges(data[('gene','interacts','gene')].edge_index)\n",
    "        else:\n",
    "            raise RuntimeError(\"No meta-path edges could be constructed.\")\n",
    "    return data\n",
    "\n",
    "# --- 3) Ù…Ø¯Ù„ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø§Ø¨Ù„ÛŒØ´Ù†\n",
    "class SimpleHANAblation(SimpleHAN):\n",
    "    def __init__(self, *args,\n",
    "                 require_phi4: bool = True,\n",
    "                 use_semantic_attention: bool = True,\n",
    "                 use_virus_token: bool = True,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.require_phi4 = require_phi4\n",
    "        self.use_semantic_attention = use_semantic_attention\n",
    "        self.use_virus_token = use_virus_token\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        xg = self.proj_gene(x_dict['gene'])\n",
    "        xv = self.proj_virus(x_dict['virus'])\n",
    "\n",
    "        phi_keys = [('gene', f'phi{i}', 'gene') for i in [1,2,3,4]]\n",
    "        phi_keys = [k for k in phi_keys if (k in edge_index_dict and edge_index_dict[k].numel() > 0)]\n",
    "        if not phi_keys:\n",
    "            raise RuntimeError(\"No Ï†-edges in edge_index_dict\")\n",
    "        if self.require_phi4:\n",
    "            if ('gene','phi4','gene') not in edge_index_dict or edge_index_dict[('gene','phi4','gene')].numel() == 0:\n",
    "                raise RuntimeError(\"Ï†4 required but missing (ablation model).\")\n",
    "\n",
    "        per_phi = []\n",
    "        for k in phi_keys:\n",
    "            phi = k[1]\n",
    "            e = edge_index_dict[k]\n",
    "            h1 = self.gat1[phi](xg, e)\n",
    "            h1 = self.norm1(F.relu(h1) + xg)\n",
    "            h1 = self.dropout(h1)\n",
    "            h2 = self.gat2[phi](h1, e)\n",
    "            h2 = self.norm2(F.relu(h2) + h1)\n",
    "            per_phi.append(h2)\n",
    "        H = torch.stack(per_phi, dim=1)\n",
    "\n",
    "        if self.use_semantic_attention:\n",
    "            fused, alpha = self.semantic_att(H)\n",
    "        else:\n",
    "            fused = H.mean(dim=1)\n",
    "\n",
    "        if self.use_post_mha:\n",
    "            # Ø§Ù…Ú©Ø§Ù† Ø­Ø°Ù virus token\n",
    "            hv_key = ('gene','hv','virus') if ('gene','hv','virus') in edge_index_dict else ('gene','interacts','virus')\n",
    "            if self.use_virus_token and hv_key in edge_index_dict and edge_index_dict[hv_key].numel() > 0:\n",
    "                zv = aggregate_virus_mean(xv, edge_index_dict[hv_key], xg.size(0))\n",
    "                tokens = torch.stack([xg, fused, zv], dim=1)\n",
    "            else:\n",
    "                tokens = torch.stack([xg, fused], dim=1)\n",
    "            fused = self.post_mha(tokens)\n",
    "\n",
    "        logit = self.cls(fused).squeeze(-1)\n",
    "        return {'gene': logit}\n",
    "\n",
    "# --- 4) Ø³Ø§Ø²Ù†Ø¯Ù‡â€ŒÛŒ Ø¯ÛŒØªØ§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø§ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ (Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÚ†Ø±)\n",
    "def build_base_heterodata_custom_features(features_df: pd.DataFrame, apply_undirected: bool = True):\n",
    "    interactions_df = pd.read_excel(FILE_GV)\n",
    "    biogrid_df      = pd.read_csv(FILE_PPI_H)\n",
    "    virus_df        = pd.read_excel(FILE_VIRUS)\n",
    "    vv_df           = pd.read_excel(FILE_PPI_VV)\n",
    "\n",
    "    for df, req in [(interactions_df, ['Genes','GenesVirus']),\n",
    "                    (biogrid_df, ['Ensembl_ID_A','Ensembl_ID_B']),\n",
    "                    (virus_df, ['GenesSymbolVirus'])]:\n",
    "        miss = [c for c in req if c not in df.columns]\n",
    "        if miss:\n",
    "            raise KeyError(f\"Missing columns {miss} in dataframe with columns {list(df.columns)}\")\n",
    "\n",
    "    y_gene = make_binary_labels(features_df['Class'])\n",
    "    interactions_df = interactions_df[\n",
    "        (interactions_df['Genes'].isin(features_df['Genes'])) &\n",
    "        (interactions_df['GenesVirus'].isin(virus_df['GenesSymbolVirus']))\n",
    "    ]\n",
    "    biogrid_df = biogrid_df[\n",
    "        (biogrid_df['Ensembl_ID_A'].isin(features_df['Genes'])) &\n",
    "        (biogrid_df['Ensembl_ID_B'].isin(features_df['Genes']))\n",
    "    ]\n",
    "\n",
    "    genes_to_index  = {g: i for i, g in enumerate(features_df['Genes'])}\n",
    "    virus_to_index  = {v: i for i, v in enumerate(virus_df['GenesSymbolVirus'])}\n",
    "\n",
    "    # ØªÙˆØ¬Ù‡: ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù‡Ù…Ù‡â€ŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬Ø² Genes/Class ÙÛŒÚ†Ø± Ù‡Ø³ØªÙ†Ø¯\n",
    "    feat_cols = [c for c in features_df.columns if c not in ['Genes','Class']]\n",
    "    X_gene  = torch.tensor(features_df[feat_cols].values, dtype=torch.float)\n",
    "    X_virus = torch.tensor(virus_df.iloc[:, 1:].values,       dtype=torch.float)\n",
    "\n",
    "    gv_edges = torch.tensor([\n",
    "        [genes_to_index[row['Genes']], virus_to_index[row['GenesVirus']]]\n",
    "        for _, row in interactions_df.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "    hh_edges = torch.tensor([\n",
    "        [genes_to_index[row['Ensembl_ID_A']], genes_to_index[row['Ensembl_ID_B']]]\n",
    "        for _, row in biogrid_df.iterrows()\n",
    "    ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    a_col = 'Official_Symbol_A_Zika'; b_col = 'Official_Symbol_B_Zika'\n",
    "    if a_col in vv_df.columns and b_col in vv_df.columns:\n",
    "        vv_edges = torch.tensor([\n",
    "            [virus_to_index.get(row[a_col], -1), virus_to_index.get(row[b_col], -1)]\n",
    "            for _, row in vv_df.iterrows()\n",
    "        ], dtype=torch.long).t().contiguous()\n",
    "        if vv_edges.numel() > 0:\n",
    "            mask_valid = (vv_edges >= 0).all(dim=0)\n",
    "            vv_edges = vv_edges[:, mask_valid]\n",
    "    else:\n",
    "        vv_edges = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data['gene'].x  = X_gene\n",
    "    data['gene'].y  = y_gene\n",
    "    data['virus'].x = X_virus\n",
    "    data['gene','interacts','virus'].edge_index = gv_edges\n",
    "    data['gene','interacts','gene' ].edge_index = hh_edges\n",
    "    if vv_edges.numel() > 0:\n",
    "        data['virus','interacts','virus'].edge_index = vv_edges\n",
    "    if apply_undirected:\n",
    "        data = T.ToUndirected()(data)\n",
    "    return data, features_df, virus_df\n",
    "\n",
    "# --- 5) Inner-CV Ø§Ø®ØªØµØ§ØµÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¨Ù„ÛŒØ´Ù† (Ø¨Ø§ Ù…Ø¯Ù„ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±)\n",
    "def inner_cv_tune_ablation(data_base: HeteroData,\n",
    "                           y: np.ndarray,\n",
    "                           outer_train_idx: np.ndarray,\n",
    "                           in_gene: int,\n",
    "                           in_virus: int,\n",
    "                           model_kwargs: dict,\n",
    "                           allow_phi: List[int],\n",
    "                           use_hh=True, use_hv=True, use_vv=True,\n",
    "                           enforce_phi4=False):\n",
    "    inner_skf = StratifiedKFold(n_splits=K_INNER, shuffle=True, random_state=SEED)\n",
    "    search_space = get_search_space()\n",
    "    best_cfg, best_auc, best_aupr, best_lists = None, -1.0, -1.0, None\n",
    "\n",
    "    for cfg in search_space:\n",
    "        aucs, auprs = [], []\n",
    "        for tr_rel, va_rel in inner_skf.split(np.zeros_like(y[outer_train_idx]), y[outer_train_idx]):\n",
    "            tr_idx = outer_train_idx[tr_rel]; va_idx = outer_train_idx[va_rel]\n",
    "\n",
    "            data_tr = induce_graph_by_genes(data_base, tr_idx)\n",
    "            data_tv = induce_graph_by_genes(data_base, np.concatenate([tr_idx, va_idx]))\n",
    "\n",
    "            data_tr = add_budget_relations_with_toggles(data_tr, use_hh, use_hv, use_vv)\n",
    "            data_tv = add_budget_relations_with_toggles(data_tv, use_hh, use_hv, use_vv)\n",
    "\n",
    "            data_tr = add_metapath_phi_edges_allowed(data_tr, allow_phi, enforce_phi4=enforce_phi4)\n",
    "            data_tv = add_metapath_phi_edges_allowed(data_tv, allow_phi, enforce_phi4=enforce_phi4)\n",
    "\n",
    "            data_tr, scg, scv = scale_features_in_fold(copy.deepcopy(data_tr), tr_idx)\n",
    "            data_tv = apply_scalers_to_data(scg, scv, data_tv)\n",
    "\n",
    "            phi_names_train = [k[1] for k in data_tr.edge_index_dict.keys()\n",
    "                               if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_tr[k].edge_index.numel()>0)]\n",
    "\n",
    "            model = SimpleHANAblation(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                                      phi_names=phi_names_train,\n",
    "                                      hidden=cfg['HIDDEN'], heads=cfg['HEADS'], dropout=cfg['DROPOUT'],\n",
    "                                      use_post_mha=model_kwargs.get('use_post_mha', True),\n",
    "                                      use_virus_token=model_kwargs.get('use_virus_token', True),\n",
    "                                      require_phi4=model_kwargs.get('require_phi4', False),\n",
    "                                      use_semantic_attention=model_kwargs.get('use_semantic_attention', True))\n",
    "\n",
    "            model = train_one_fold_inductive(model, data_tr, data_tv, tr_idx, va_idx,\n",
    "                                             lr=cfg['LR'], max_epochs=MAX_EPOCHS,\n",
    "                                             weight_decay=cfg['WEIGHT_DECAY'], patience=PATIENCE)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v_logits = model(data_tv.x_dict, data_tv.edge_index_dict)['gene'][torch.as_tensor(va_idx, device=DEVICE)]\n",
    "                v_prob   = torch.sigmoid(v_logits).cpu().numpy()\n",
    "                v_true   = y[va_idx]\n",
    "            aucs.append(roc_auc_score(v_true, v_prob))\n",
    "            auprs.append(average_precision_score(v_true, v_prob))\n",
    "\n",
    "        if len(aucs) == 0:\n",
    "            continue\n",
    "        m_auc, m_aupr = float(np.mean(aucs)), float(np.mean(auprs))\n",
    "        if (m_auc > best_auc) or (np.isclose(m_auc, best_auc) and m_aupr > best_aupr):\n",
    "            best_auc, best_aupr = m_auc, m_aupr\n",
    "            best_cfg, best_lists = cfg, (aucs, auprs)\n",
    "\n",
    "    if best_cfg is None:\n",
    "        best_cfg = dict(HIDDEN=256, HEADS=4, DROPOUT=0.3, LR=1e-3, WEIGHT_DECAY=1e-4)\n",
    "        return best_cfg, dict(auc_mean=np.nan, auc_sd=np.nan, aupr_mean=np.nan, aupr_sd=np.nan)\n",
    "\n",
    "    aucs, auprs = best_lists\n",
    "    return best_cfg, dict(auc_mean=np.mean(aucs), auc_sd=np.std(aucs),\n",
    "                          aupr_mean=np.mean(auprs), aupr_sd=np.std(auprs))\n",
    "\n",
    "# --- 6) Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø§Ø¨Ù„ÛŒØ´Ù† Ùˆ Ø«Ø¨Øª Ø®Ø±ÙˆØ¬ÛŒ outer-test\n",
    "@dataclass\n",
    "class AblationCfg:\n",
    "    name: str\n",
    "    allow_phi: List[int]         # e.g., [1,2,3,4] or [1,2,3]\n",
    "    use_hh: bool = True\n",
    "    use_hv: bool = True\n",
    "    use_vv: bool = True\n",
    "    require_phi4: bool = True\n",
    "    use_semantic_attention: bool = True\n",
    "    use_post_mha: bool = True\n",
    "    use_virus_token: bool = True\n",
    "    drop_feature_groups: List[str] = None  # e.g., ['ppi_topology']\n",
    "\n",
    "def run_single_ablation(cfg: AblationCfg, out_csv_path: str):\n",
    "    print(f\"\\n===== Ablation: {cfg.name} =====\")\n",
    "\n",
    "    # 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø­Ø°Ù ÙÛŒÚ†Ø±Ù‡Ø§ (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)\n",
    "    feat_df = pd.read_csv(FILE_GENE)\n",
    "    if cfg.drop_feature_groups:\n",
    "        feat_df = drop_feature_groups(feat_df, cfg.drop_feature_groups)\n",
    "    data_base, genes_df, virus_df = build_base_heterodata_custom_features(feat_df, apply_undirected=True)\n",
    "\n",
    "    # 2) Ø§Ø¹Ù…Ø§Ù„ ÛŒØ§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² + Ù…ØªØ§Ù¾Ø«â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø±ÙˆÛŒ Ú¯Ø±Ø§Ù Ú©Ø§Ù…Ù„ (Ø¨Ø±Ø§ÛŒ outer-test)\n",
    "    data_full = copy.deepcopy(data_base)\n",
    "    data_full = add_budget_relations_with_toggles(data_full, use_hh=cfg.use_hh, use_hv=cfg.use_hv, use_vv=cfg.use_vv)\n",
    "    data_full = add_metapath_phi_edges_allowed(data_full, cfg.allow_phi, enforce_phi4=cfg.require_phi4)\n",
    "\n",
    "    in_gene  = data_base['gene'].x.size(1)\n",
    "    in_virus = data_base['virus'].x.size(1)\n",
    "    y = data_base['gene'].y.cpu().numpy()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    rows = []\n",
    "\n",
    "    for fold, (outer_train_idx, outer_test_idx) in enumerate(skf.split(np.zeros_like(y), y), 1):\n",
    "        # --- tuning Ø¨Ø§ inner-CV Ø±ÙˆÛŒ outer-train\n",
    "        best_cfg, inner_stats = inner_cv_tune_ablation(\n",
    "            data_base, y, outer_train_idx, in_gene, in_virus,\n",
    "            model_kwargs=dict(require_phi4=cfg.require_phi4,\n",
    "                              use_semantic_attention=cfg.use_semantic_attention,\n",
    "                              use_post_mha=cfg.use_post_mha,\n",
    "                              use_virus_token=cfg.use_virus_token),\n",
    "            allow_phi=cfg.allow_phi,\n",
    "            use_hh=cfg.use_hh, use_hv=cfg.use_hv, use_vv=cfg.use_vv,\n",
    "            enforce_phi4=cfg.require_phi4\n",
    "        )\n",
    "\n",
    "        # split outer-train -> inner_train + outer-valid\n",
    "        inner_train_idx, outer_valid_idx = train_test_split(\n",
    "            outer_train_idx, test_size=0.2, random_state=SEED, stratify=y[outer_train_idx]\n",
    "        )\n",
    "\n",
    "        # Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´/ÙˆÙ„ÛŒØ¯ + Ø¨ÙˆØ¯Ø¬Ù‡â€ŒÙ‡Ø§/Ù…ØªØ§Ù¾Ø«â€ŒÙ‡Ø§\n",
    "        data_train = induce_graph_by_genes(data_base, inner_train_idx)\n",
    "        data_tv    = induce_graph_by_genes(data_base, np.concatenate([inner_train_idx, outer_valid_idx]))\n",
    "        data_train = add_budget_relations_with_toggles(data_train, cfg.use_hh, cfg.use_hv, cfg.use_vv)\n",
    "        data_tv    = add_budget_relations_with_toggles(data_tv,    cfg.use_hh, cfg.use_hv, cfg.use_vv)\n",
    "        data_train = add_metapath_phi_edges_allowed(data_train, cfg.allow_phi, enforce_phi4=cfg.require_phi4)\n",
    "        data_tv    = add_metapath_phi_edges_allowed(data_tv,    cfg.allow_phi, enforce_phi4=cfg.require_phi4)\n",
    "\n",
    "        data_train, scg, scv = scale_features_in_fold(copy.deepcopy(data_train), inner_train_idx)\n",
    "        data_tv    = apply_scalers_to_data(scg, scv, data_tv)\n",
    "        data_full_ = apply_scalers_to_data(scg, scv, copy.deepcopy(data_full))\n",
    "\n",
    "        phi_names_train = [k[1] for k in data_train.edge_index_dict.keys()\n",
    "                           if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_train[k].edge_index.numel()>0)]\n",
    "\n",
    "        model = SimpleHANAblation(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                                  phi_names=phi_names_train,\n",
    "                                  hidden=best_cfg['HIDDEN'], heads=best_cfg['HEADS'], dropout=best_cfg['DROPOUT'],\n",
    "                                  use_post_mha=cfg.use_post_mha, use_virus_token=cfg.use_virus_token,\n",
    "                                  require_phi4=cfg.require_phi4,\n",
    "                                  use_semantic_attention=cfg.use_semantic_attention)\n",
    "\n",
    "        model = train_one_fold_inductive(model, data_train, data_tv,\n",
    "                                         inner_train_idx, outer_valid_idx,\n",
    "                                         lr=best_cfg['LR'], max_epochs=MAX_EPOCHS,\n",
    "                                         weight_decay=best_cfg['WEIGHT_DECAY'], patience=PATIENCE)\n",
    "\n",
    "        # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ outer-valid\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_logits = model(data_tv.x_dict, data_tv.edge_index_dict)['gene'][torch.as_tensor(outer_valid_idx, device=DEVICE)]\n",
    "            v_prob   = torch.sigmoid(v_logits).cpu().numpy()\n",
    "            v_true   = y[outer_valid_idx]\n",
    "        thr = find_best_threshold(v_true, v_prob, grid=201, metric='f1')\n",
    "\n",
    "        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ outer-test (Ú¯Ø±Ø§Ù Ú©Ø§Ù…Ù„Ù Ù…Ù‚ÛŒØ§Ø³â€ŒØ´Ø¯Ù‡)\n",
    "        test_res_best, _, _ = evaluate_split(model, data_full_, outer_test_idx, thr=thr)\n",
    "        rows.append(dict(\n",
    "            virus=VIRUS_NAME, ablation=cfg.name, outer_fold=fold,\n",
    "            AUROC=test_res_best['auc_roc'],\n",
    "            AUPRC=test_res_best['auc_pr'],\n",
    "            F1=test_res_best['f1'],\n",
    "            Acc=test_res_best['accuracy'],\n",
    "            Prec=test_res_best['precision'],\n",
    "            Rec=test_res_best['recall'],\n",
    "            Brier=test_res_best['brier'],\n",
    "            thr=thr,\n",
    "            HIDDEN=best_cfg['HIDDEN'], HEADS=best_cfg['HEADS'],\n",
    "            DROPOUT=best_cfg['DROPOUT'], LR=best_cfg['LR'],\n",
    "            WD=best_cfg['WEIGHT_DECAY']\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "    print(f\"[Ablation:{cfg.name}] saved -> {out_csv_path}\")\n",
    "    # Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡â€ŒÛŒ Ø¬Ø¯ÙˆÙ„ (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†Â±SD Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡)\n",
    "    for m in ['AUROC','AUPRC','F1','Acc','Prec','Rec','Brier']:\n",
    "        print(f\"{m}: {df[m].mean():.3f} Â± {df[m].std():.3f}\")\n",
    "    return df\n",
    "\n",
    "# --- 7) ÙÙ‡Ø±Ø³Øª Ø§Ø¨Ù„ÛŒØ´Ù†â€ŒÙ‡Ø§ Ùˆ Ù„Ø§Ù†Ú†Ø± Ø¬Ø¯ÙˆÙ„ 3\n",
    "def run_ablation_table3():\n",
    "    out_path = f\"ablation_table3_{VIRUS_NAME}.csv\"\n",
    "    ablations = [\n",
    "        AblationCfg(name=\"Full\",      allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    use_semantic_attention=True, use_post_mha=True, use_virus_token=True,\n",
    "                    drop_feature_groups=[]),\n",
    "        AblationCfg(name=\"-phi4\",     allow_phi=[1,2,3],   require_phi4=False,\n",
    "                    use_semantic_attention=True, use_post_mha=True, use_virus_token=True),\n",
    "        AblationCfg(name=\"NoSemAttn\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    use_semantic_attention=False, use_post_mha=True, use_virus_token=True),\n",
    "        AblationCfg(name=\"-PostMHA\",  allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    use_semantic_attention=True, use_post_mha=False, use_virus_token=True),\n",
    "        AblationCfg(name=\"-VirusTok\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    use_semantic_attention=True, use_post_mha=True, use_virus_token=False),\n",
    "        # Feature ablations\n",
    "        AblationCfg(name=\"âˆ’Topology\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['ppi_topology']),\n",
    "        AblationCfg(name=\"âˆ’Node2Vec/DeepLoc\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['emb_loc']),\n",
    "        AblationCfg(name=\"âˆ’GO/Pathway\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['go_pathway']),\n",
    "        AblationCfg(name=\"âˆ’Sequence\",  allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['sequence']),\n",
    "        AblationCfg(name=\"âˆ’Conservation\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['conservation']),\n",
    "        AblationCfg(name=\"âˆ’Domains\", allow_phi=[1,2,3,4], require_phi4=True,\n",
    "                    drop_feature_groups=['domains_motifs']),\n",
    "    ]\n",
    "\n",
    "    all_rows = []\n",
    "    for cfg in ablations:\n",
    "        df = run_single_ablation(cfg, out_csv_path=f\"ablt_{cfg.name}_{VIRUS_NAME}.csv\")\n",
    "        all_rows.append(df)\n",
    "    big = pd.concat(all_rows, axis=0).reset_index(drop=True)\n",
    "    big.to_csv(out_path, index=False)\n",
    "    print(f\"\\n=== TABLE 3 (outer-test, all ablations) -> {out_path} ===\")\n",
    "\n",
    "# --- ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ\n",
    "# run_ablation_table3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import glob\n",
    "import warnings\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ø§Ø´Øª (Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø´Ø¯)\n",
    "try:\n",
    "    import mygene  # pip install mygene\n",
    "except Exception:\n",
    "    mygene = None\n",
    "    warnings.warn(\"mygene Ù†ØµØ¨ Ù†ÛŒØ³ØªØ› Ø§Ú¯Ø± Ù†Ú¯Ø§Ø´Øª Ù„Ø§Ø²Ù… Ø´ÙˆØ¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª join Ú©Ø§Ù…Ù„ Ù†Ø´ÙˆØ¯.\")\n",
    "\n",
    "# -----------------------------\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø³ÛŒØ±Ù‡Ø§\n",
    "# -----------------------------\n",
    "# Ù¾ÙˆØ´Ù‡Ù” RF Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ú©Ø¯ RF ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ (Ù‡Ù…Ø§Ù† ROOT/PRED_DIR)\n",
    "RF_ROOT   = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/HRF/Publications\"\n",
    "RF_PRED_DIR = os.path.join(RF_ROOT, \"_PREDICTIONS\")\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ HAN (Ø®Ø±ÙˆØ¬ÛŒ ØªØ§Ø¨Ø¹ run_fit_and_predict_all Ø¯Ø± Ú©Ø¯ HAN)\n",
    "HAN_PRED_CSV = \"Wholedata-HAN-predictions-Zika-final.csv\"\n",
    "\n",
    "# ÙØ§ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡Ù” Ú˜Ù†â€ŒÙ‡Ø§ (ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø¯ HAN)\n",
    "FILE_GENE_LABELED = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/Zika/ZikaInputdataForDeepL500HDF1000Non39Features_WithClass.csv\"\n",
    "\n",
    "# Ú¯Ø²ÛŒÙ†Ù‡Ù” ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒØ±ÙˆØ³ Ø¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„ RFØ› Ø§Ú¯Ø± None Ø¨Ø§Ø´Ø¯ Ù‡Ù…Ù‡ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯\n",
    "VIRUS_KEYWORD_IN_RF_FILENAMES = \"Zika\"  # ÛŒØ§ None\n",
    "\n",
    "# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ\n",
    "OUT_DIR = \"_ENSEMBLE_OUT\"\n",
    "OSUM_CV_METRICS = os.path.join(OUT_DIR, \"ensemble_cv_metrics.csv\")\n",
    "OJSON_WEIGHTS   = os.path.join(OUT_DIR, \"ensemble_selected_weights.json\")\n",
    "OPRED_ALL       = os.path.join(OUT_DIR, \"ensemble_predictions_all_genes.csv\")\n",
    "OPRED_LABELED   = os.path.join(OUT_DIR, \"ensemble_predictions_labeled_genes.csv\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Ú©Ù…Ú©ÛŒâ€ŒÙ‡Ø§\n",
    "# -----------------------------\n",
    "\n",
    "def _read_rf_pred_tsv_gz(path: str) -> pd.DataFrame:\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        df = pd.read_csv(f, sep='\\t')\n",
    "    # Ø§Ù†ØªØ¸Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: Ensembl_ID, Prob_HDF, Rank (Ùˆ ...)\n",
    "    needed = ['Ensembl_ID', 'Prob_HDF']\n",
    "    miss = [c for c in needed if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ {miss} Ø¯Ø± {path} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n",
    "    return df[['Ensembl_ID','Prob_HDF']].copy()\n",
    "\n",
    "\n",
    "def load_rf_predictions(pred_dir: str = RF_PRED_DIR,\n",
    "                        virus_keyword: Optional[str] = VIRUS_KEYWORD_IN_RF_FILENAMES) -> pd.DataFrame:\n",
    "    pats = glob.glob(os.path.join(pred_dir, \"*.tsv.gz\"))\n",
    "    if virus_keyword:\n",
    "        files = [p for p in pats if re.search(virus_keyword, os.path.basename(p), flags=re.IGNORECASE)]\n",
    "        if not files:\n",
    "            # Ø§Ú¯Ø± Ø¯Ø± Ù†Ø§Ù… Ù†Ø¨ÙˆØ¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ±\n",
    "            files = pats\n",
    "    else:\n",
    "        files = pats\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ RF Ø¯Ø± {pred_dir} ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "\n",
    "    dfs = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            df = _read_rf_pred_tsv_gz(p)\n",
    "            df.rename(columns={'Prob_HDF': f'Prob_RF_{os.path.splitext(os.path.basename(p))[0]}'}, inplace=True)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ù¾Ø±Ø´ ÙØ§ÛŒÙ„ RF ({os.path.basename(p)}): {e}\")\n",
    "            continue\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ RF Ù…Ø¹ØªØ¨Ø± Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù†Ø´Ø¯.\")\n",
    "\n",
    "    # merge ØªØ¯Ø±ÛŒØ¬ÛŒ Ø±ÙˆÛŒ Ensembl_ID\n",
    "    base = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        base = base.merge(d, on='Ensembl_ID', how='outer')\n",
    "\n",
    "    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒ Ù‡Ù…Ù‡Ù” Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Prob_RF_*\n",
    "    prob_cols = [c for c in base.columns if c.startswith('Prob_RF_')]\n",
    "    base['Prob_RF'] = base[prob_cols].mean(axis=1)\n",
    "    return base[['Ensembl_ID','Prob_RF']].copy()\n",
    "\n",
    "\n",
    "def load_han_predictions(csv_path: str = HAN_PRED_CSV) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Ø§Ù†ØªØ¸Ø§Ø±: Ø³ØªÙˆÙ† \"Gene_Name\" Ùˆ \"Predicted_Probability\"\n",
    "    # Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ø®Ø±ÙˆØ­ÛŒâ€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ú©Ù…ÛŒ Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯\n",
    "    gcol = None\n",
    "    for cand in ['Gene_Name','Genes','Gene','Ensembl_ID']:\n",
    "        if cand in df.columns:\n",
    "            gcol = cand\n",
    "            break\n",
    "    if gcol is None:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ† Ù†Ø§Ù… Ú˜Ù† Ø¯Ø± {csv_path} ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "    pcol = None\n",
    "    for cand in ['Predicted_Probability','Prob','Probability','Prob_HDF','Score']:\n",
    "        if cand in df.columns:\n",
    "            pcol = cand\n",
    "            break\n",
    "    if pcol is None:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ø¯Ø± {csv_path} ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "\n",
    "    out = df[[gcol, pcol]].copy()\n",
    "    out.columns = ['Gene_Key', 'Prob_HAN']\n",
    "\n",
    "    # Ø§Ú¯Ø± Ø¨Ù‡ Ù†Ø¸Ø± Ensembl ID Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ù…Ø§Ù† Ø±Ø§ Ensembl_ID Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ…\n",
    "    if out['Gene_Key'].astype(str).str.startswith(('ENSG','ENSMUSG','ENS')).mean() > 0.8:\n",
    "        out.rename(columns={'Gene_Key':'Ensembl_ID'}, inplace=True)\n",
    "        return out[['Ensembl_ID','Prob_HAN']]\n",
    "\n",
    "    # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†ØµÙˆØ±Øª Ø³Ø¹ÛŒ Ø¯Ø± Ù†Ú¯Ø§Ø´Øª Symbolâ†’Ensembl (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)\n",
    "    if mygene is None:\n",
    "        warnings.warn(\"mygene Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªØ› Ù†Ú¯Ø§Ø´Øª symbolâ†’ensembl Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯. Ø³ØªÙˆÙ† Gene_Key Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
    "        out.rename(columns={'Gene_Key':'Ensembl_ID'}, inplace=True)  # Ø§Ù…ÛŒØ¯ Ø¨Ù‡ Ø§ÛŒÙ†â€ŒÚ©Ù‡ Ù‡Ù…Ø§Ù† Ø¨Ø§Ø´Ø¯\n",
    "        return out[['Ensembl_ID','Prob_HAN']]\n",
    "\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    syms = out['Gene_Key'].astype(str).tolist()\n",
    "    res = mg.querymany(syms, scopes='symbol,alias', fields='ensembl.gene', species='human', as_dataframe=False, returnall=False)\n",
    "    m: Dict[str, str] = {}\n",
    "    for r in res:\n",
    "        q = r.get('query'); ens = r.get('ensembl')\n",
    "        if not q:\n",
    "            continue\n",
    "        ids = []\n",
    "        if isinstance(ens, list):\n",
    "            ids = [str(x.get('gene')) for x in ens if isinstance(x, dict) and x.get('gene')]\n",
    "        elif isinstance(ens, dict):\n",
    "            if ens.get('gene'):\n",
    "                ids = [str(ens.get('gene'))]\n",
    "        ids = [i for i in ids if i]\n",
    "        if ids:\n",
    "            m[q] = sorted(set(ids))[0]\n",
    "    out['Ensembl_ID'] = out['Gene_Key'].map(m)\n",
    "    out = out.dropna(subset=['Ensembl_ID']).copy()\n",
    "    return out[['Ensembl_ID','Prob_HAN']]\n",
    "\n",
    "\n",
    "def load_labeled_genes(file_path: str = FILE_GENE_LABELED) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Ø§Ù†ØªØ¸Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: Genes, Class\n",
    "    need = ['Genes','Class']\n",
    "    miss = [c for c in need if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ {miss} Ø¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "    out = df[['Genes','Class']].copy()\n",
    "    out.rename(columns={'Genes':'Ensembl_ID'}, inplace=True)\n",
    "    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§: HDF=1, non-HDF=0\n",
    "    y = out['Class'].astype(str).str.replace('-', '').str.lower()\n",
    "    out['y'] = np.where(y.eq('hdf'), 1, 0)\n",
    "    return out[['Ensembl_ID','y']]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Stacking & Weighted Average\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate_metrics(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5) -> Dict[str,float]:\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        'AUC': roc_auc_score(y_true, y_prob),\n",
    "        'AUPR': average_precision_score(y_true, y_prob),\n",
    "        'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def cv_meta_logreg(X: np.ndarray, y: np.ndarray, Cs: List[float] = [0.01,0.1,1,3,10,30], n_splits: int = 5, seed: int = 42) -> Tuple[LogisticRegression, Dict[str,float]]:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    best_auc, best_C = -1.0, None\n",
    "    for C in Cs:\n",
    "        aucs = []\n",
    "        for tr, te in skf.split(X, y):\n",
    "            lr = LogisticRegression(C=C, solver='liblinear')\n",
    "            lr.fit(X[tr], y[tr])\n",
    "            p = lr.predict_proba(X[te])[:,1]\n",
    "            aucs.append(roc_auc_score(y[te], p))\n",
    "        m = float(np.mean(aucs))\n",
    "        if m > best_auc:\n",
    "            best_auc, best_C = m, C\n",
    "    # fit final\n",
    "    lr_final = LogisticRegression(C=best_C, solver='liblinear')\n",
    "    lr_final.fit(X, y)\n",
    "    return lr_final, {'method':'stacking_logreg','best_C':best_C,'cv_auc':best_auc}\n",
    "\n",
    "\n",
    "def cv_best_weight_average(p1: np.ndarray, p2: np.ndarray, y: np.ndarray, grid: int = 101, n_splits: int = 5, seed: int = 42) -> Tuple[float, Dict[str,float]]:\n",
    "    # ÙˆØ²Ù† w Ø¨Ø±Ø§ÛŒ HAN: p = w*p1 + (1-w)*p2\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    ws = np.linspace(0.0, 1.0, grid)\n",
    "    best_auc, best_w = -1.0, 0.5\n",
    "    for w in ws:\n",
    "        aucs = []\n",
    "        for tr, te in skf.split(p1, y):\n",
    "            pp = w*p1[tr] + (1-w)*p2[tr]\n",
    "            aucs.append(roc_auc_score(y[tr], pp))\n",
    "        m = float(np.mean(aucs))\n",
    "        if m > best_auc:\n",
    "            best_auc, best_w = m, float(w)\n",
    "    return best_w, {'method':'weighted_average','best_w':best_w,'cv_auc':best_auc}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
    "    han = load_han_predictions(HAN_PRED_CSV)\n",
    "    rf  = load_rf_predictions(RF_PRED_DIR, VIRUS_KEYWORD_IN_RF_FILENAMES)\n",
    "\n",
    "    # 2) join Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ú˜Ù†â€ŒÙ‡Ø§ (Ù¾ÙˆØ´Ø´ Ù…Ø´ØªØ±Ú© Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† Ù‡Ø± Ú©Ø¯Ø§Ù… Ú©Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª)\n",
    "    all_pred = pd.merge(han, rf, on='Ensembl_ID', how='outer')\n",
    "\n",
    "    # 3) Ø¯Ø§Ø¯Ù‡Ù” Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ensemble\n",
    "    lab = load_labeled_genes(FILE_GENE_LABELED)\n",
    "    L = all_pred.merge(lab, on='Ensembl_ID', how='inner').dropna(subset=['Prob_HAN','Prob_RF'])\n",
    "    if L.shape[0] < 20:\n",
    "        raise RuntimeError(\"ØªÙ„Ø§Ù‚ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ú©Ù… Ø§Ø³Øª. Ù…Ø³ÛŒØ±Ù‡Ø§/Ø¯ÛŒØªØ§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
    "\n",
    "    X = L[['Prob_HAN','Prob_RF']].values.astype(float)\n",
    "    y = L['y'].values.astype(int)\n",
    "\n",
    "    # 4) Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ensemble Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø± Ø§Ø³Ø§Ø³ CV-AUC\n",
    "    lr_model, info_lr = cv_meta_logreg(X, y)\n",
    "    w_best, info_w = cv_best_weight_average(X[:,0], X[:,1], y)\n",
    "\n",
    "    # 5) Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†\n",
    "    if info_lr['cv_auc'] >= info_w['cv_auc']:\n",
    "        chosen = {'chosen':'stacking_logreg', **info_lr}\n",
    "        # Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ labeled\n",
    "        L['Prob_Ensemble'] = lr_model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "        chosen = {'chosen':'weighted_average', **info_w}\n",
    "        L['Prob_Ensemble'] = w_best*X[:,0] + (1.0-w_best)*X[:,1]\n",
    "\n",
    "    # 6) Ú¯Ø²Ø§Ø±Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡Ù” Ù†ØªØ§ÛŒØ¬ Ø±ÙˆÛŒ labeled\n",
    "    met_lab = evaluate_metrics(L['y'].values, L['Prob_Ensemble'].values, thr=0.5)\n",
    "    cv_rows = [\n",
    "        {'Variant':'HAN_only','AUC':roc_auc_score(y, X[:,0])},\n",
    "        {'Variant':'RF_only','AUC':roc_auc_score(y, X[:,1])},\n",
    "        {'Variant':'Stacking(LR)','AUC':info_lr['cv_auc']},\n",
    "        {'Variant':'WeightedAvg','AUC':info_w['cv_auc']},\n",
    "        {'Variant':'Chosen@0.5(AUC)', 'AUC':met_lab['AUC']},\n",
    "    ]\n",
    "    pd.DataFrame(cv_rows).to_csv(OSUM_CV_METRICS, index=False)\n",
    "\n",
    "    with open(OJSON_WEIGHTS, 'w') as f:\n",
    "        json.dump(chosen, f, indent=2)\n",
    "\n",
    "    L[['Ensembl_ID','Prob_HAN','Prob_RF','Prob_Ensemble','y']].to_csv(OPRED_LABELED, index=False)\n",
    "\n",
    "    print(\"=== Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ensemble ===\")\n",
    "    print(json.dumps(chosen, indent=2))\n",
    "    print(\"\\n=== Ú©Ø§Ø±Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡Ù” Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡ (threshold=0.5) ===\")\n",
    "    for k,v in met_lab.items():\n",
    "        print(f\"{k:>10s}: {v:.4f}\")\n",
    "\n",
    "    # 7) Ø§Ø¹Ù…Ø§Ù„ Ø±ÙˆÛŒ ØªÙ…Ø§Ù… Ú˜Ù†â€ŒÙ‡Ø§\n",
    "    A = all_pred.copy()\n",
    "    # Ø§Ú¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª NaN Ø¨ÙˆØ¯ØŒ Ø¨Ø§ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´ÙˆØ¯ (fallback)\n",
    "    if A['Prob_HAN'].isna().any() and 'Prob_RF' in A:\n",
    "        A.loc[A['Prob_HAN'].isna() & A['Prob_RF'].notna(), 'Prob_HAN'] = A.loc[A['Prob_HAN'].isna() & A['Prob_RF'].notna(), 'Prob_RF']\n",
    "    if A['Prob_RF'].isna().any() and 'Prob_HAN' in A:\n",
    "        A.loc[A['Prob_RF'].isna() & A['Prob_HAN'].notna(), 'Prob_RF'] = A.loc[A['Prob_RF'].isna() & A['Prob_HAN'].notna(), 'Prob_HAN']\n",
    "\n",
    "    # Ù‡Ø± Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¨Ø§ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯\n",
    "    A = A.dropna(subset=['Prob_HAN','Prob_RF']).copy()\n",
    "\n",
    "    X_all = A[['Prob_HAN','Prob_RF']].values.astype(float)\n",
    "    if chosen['chosen']=='stacking_logreg':\n",
    "        A['Prob_Ensemble'] = lr_model.predict_proba(X_all)[:,1]\n",
    "    else:\n",
    "        A['Prob_Ensemble'] = chosen['best_w']*X_all[:,0] + (1.0-chosen['best_w'])*X_all[:,1]\n",
    "\n",
    "    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡\n",
    "    A = A.sort_values('Prob_Ensemble', ascending=False).reset_index(drop=True)\n",
    "    A['Rank'] = np.arange(1, len(A)+1)\n",
    "    A.to_csv(OPRED_ALL, index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OSUM_CV_METRICS}\\nâœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OJSON_WEIGHTS}\\nâœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OPRED_LABELED}\\nâœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OPRED_ALL}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble (Stacking) without leakage for RF + HAN.\n",
    "\n",
    "- Builds the same 10-fold stratified splits once.\n",
    "- 8:1:1 train/val/test per fold.\n",
    "\n",
    "- RF is trained inline in this script (no leakage; preprocessors fitted on train only).\n",
    "- HAN is called via your han_hdf_han_final_cv.py utilities in inductive mode (no transductive leakage).\n",
    "\n",
    "Outputs:\n",
    "  ./_ENSEMBLE_OUT/\n",
    "    oof_val_table.csv          # OOF predictions (val) per fold for RF/HAN + y\n",
    "    perfold_test_table.csv     # test predictions per fold for RF/HAN + y\n",
    "    stacking_meta_info.json    # chosen C, splits, scheme, seeds\n",
    "    stacking_metrics_test.csv  # aggregated test metrics (AUC/AUPR/... for base & ensemble)\n",
    "    ensemble_predictions_all_genes.csv  # final ensemble probs for ALL genes (using final HAN+RF preds)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, math, warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Config (adjust paths)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "N_SPLITS = 10\n",
    "SPLIT_SCHEME = \"8:1:1\"   # \"8:1:1\" (paper) or \"han_5x5\" (mirror HAN code)\n",
    "DEVICE_INDEX = 0         # GPU index for HAN\n",
    "\n",
    "# Labeled gene table (features + Class)\n",
    "FILE_GENE = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/Zika/ZikaInputdataForDeepL500HDF1000Non39Features_WithClass.csv\"\n",
    "\n",
    "# Final full predictions (for applying ensemble to ALL genes)\n",
    "HAN_PRED_CSV = \"Wholedata-HAN-predictions-Zika-final.csv\"  # from run_fit_and_predict_all()\n",
    "# RF predictions over whole genome: folder with many *.tsv.gz (prob per gene); weâ€™ll average them\n",
    "RF_PRED_DIR  = \"/media/mohadeseh/d2987156-83a1-4537-b507-30f08b63b454/Naseri/FinalFolder/HRF/Publications/_PREDICTIONS\"\n",
    "RF_FILENAME_FILTER_KEYWORD = \"Zika\"  # filter filenames by keyword; set None to take all\n",
    "\n",
    "OUT_DIR = \"_ENSEMBLE_OUT\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Imports from your HAN code\n",
    "# -------------------------\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, f1_score,\n",
    "                             precision_score, recall_score, accuracy_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use your han utilities\n",
    "from han_hdf_han_final_cv import (\n",
    "    build_base_heterodata, add_budget_relations, add_metapath_phi_edges,\n",
    "    induce_graph_by_genes, scale_features_in_fold, apply_scalers_to_data,\n",
    "    SimpleHAN, train_one_fold_inductive, evaluate_split, find_best_threshold,\n",
    "    INCLUDE_VV_METAPATH, MIN_VV_EDGES_FOR_USE, HIDDEN, HEADS, DROPOUT, LR, MAX_EPOCHS, WEIGHT_DECAY, PATIENCE\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(f\"cuda:{DEVICE_INDEX}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Small utils\n",
    "# -------------------------\n",
    "def set_seeds(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def evaluate_metrics_bin(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5) -> Dict[str, float]:\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return dict(\n",
    "        AUC   = roc_auc_score(y_true, y_prob),\n",
    "        AUPR  = average_precision_score(y_true, y_prob),\n",
    "        F1    = f1_score(y_true, y_pred, zero_division=0),\n",
    "        Precision = precision_score(y_true, y_pred, zero_division=0),\n",
    "        Recall    = recall_score(y_true, y_pred, zero_division=0),\n",
    "        Accuracy  = accuracy_score(y_true, y_pred),\n",
    "    )\n",
    "\n",
    "def load_labeled_df(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if 'Genes' not in df.columns or 'Class' not in df.columns:\n",
    "        raise RuntimeError(\"Expected columns 'Genes' and 'Class' in labeled file.\")\n",
    "    # y: 1=HDF, 0=nonHDF\n",
    "    s = df['Class'].astype(str).str.replace('-', '').str.lower()\n",
    "    y = np.where(s.eq('hdf'), 1, 0).astype(int)\n",
    "    return pd.DataFrame({'Ensembl_ID': df['Genes'].astype(str), 'y': y}), df\n",
    "\n",
    "# -------------------------\n",
    "# RF â€” per-fold training producing OOF (val) and test preds (no leakage)\n",
    "# -------------------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def near_zero_var_mask(X: pd.DataFrame, unique_cut: float = 10.0, freq_cut: float = 19.0) -> pd.Series:\n",
    "    n = X.shape[0]\n",
    "    mask = pd.Series(False, index=X.columns)\n",
    "    for c in X.columns:\n",
    "        v = X[c].astype(str).fillna('NA')\n",
    "        counts = v.value_counts(dropna=False)\n",
    "        if len(counts) <= 1:\n",
    "            mask[c] = True; continue\n",
    "        percent_unique = 100.0 * len(counts) / n\n",
    "        most = counts.iloc[0]\n",
    "        second = counts.iloc[1] if len(counts) > 1 else 0.5\n",
    "        freq_ratio = (most / max(second, 1e-9)) if second > 0 else np.inf\n",
    "        if percent_unique <= unique_cut and freq_ratio >= freq_cut:\n",
    "            mask[c] = True\n",
    "    return mask\n",
    "\n",
    "def drop_high_correlation(X: pd.DataFrame, cutoff: float = 0.70) -> List[str]:\n",
    "    if X.shape[1] <= 1: return list(X.columns)\n",
    "    corr = X.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > cutoff)]\n",
    "    keep = [c for c in X.columns if c not in to_drop]\n",
    "    return keep or list(X.columns)\n",
    "\n",
    "def train_rf_fold_return_probs(X: pd.DataFrame, y: np.ndarray,\n",
    "                               train_idx, val_idx, test_idx,\n",
    "                               seed: int = SEED) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Preprocess fit on train only\n",
    "    X_train = X.iloc[train_idx].copy()\n",
    "    X_val   = X.iloc[val_idx].copy()\n",
    "    X_test  = X.iloc[test_idx].copy()\n",
    "\n",
    "    # NZV\n",
    "    nzv = near_zero_var_mask(X_train)\n",
    "    cols = [c for c in X_train.columns if not nzv[c]]\n",
    "    X_train, X_val, X_test = X_train[cols], X_val[cols], X_test[cols]\n",
    "\n",
    "    # Impute + Scale\n",
    "    pp = Pipeline([('imp', SimpleImputer(strategy='median')),\n",
    "                   ('sc', StandardScaler())])\n",
    "    Xtr = pd.DataFrame(pp.fit_transform(X_train), index=X_train.index, columns=cols)\n",
    "    Xva = pd.DataFrame(pp.transform(X_val),   index=X_val.index,   columns=cols)\n",
    "    Xte = pd.DataFrame(pp.transform(X_test),  index=X_test.index,  columns=cols)\n",
    "\n",
    "    # Drop highly correlated (on train only)\n",
    "    sel = drop_high_correlation(Xtr, cutoff=0.70)\n",
    "    Xtr, Xva, Xte = Xtr[sel], Xva[sel], Xte[sel]\n",
    "\n",
    "    # Simple RF (strong, stable)\n",
    "    p = Xtr.shape[1]\n",
    "    mtry = max(1, int(round(math.sqrt(p))))\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=800, max_features=mtry, random_state=seed, n_jobs=-1, class_weight=None\n",
    "    )\n",
    "    rf.fit(Xtr, y[train_idx])\n",
    "\n",
    "    # Prob for positive class (HDF=1)\n",
    "    prob_val  = rf.predict_proba(Xva)[:, 1]\n",
    "    prob_test = rf.predict_proba(Xte)[:, 1]\n",
    "    return prob_val, prob_test\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# HAN â€” per-fold OOF (val) and test probs via your inductive pipeline\n",
    "# -------------------------\n",
    "def han_fold_return_probs(data_base,\n",
    "                          in_gene: int, in_virus: int, y_all: np.ndarray,\n",
    "                          train_idx, val_idx, test_idx,\n",
    "                          seed: int = SEED) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Build graphs\n",
    "    data_train    = induce_graph_by_genes(data_base, np.asarray(train_idx))\n",
    "    data_trainval = induce_graph_by_genes(data_base, np.asarray(list(train_idx) + list(val_idx)))\n",
    "\n",
    "    data_full = build_full_graph_scaled_later(data_base)\n",
    "\n",
    "    # scale (fit on train_idx genes of data_train)\n",
    "    data_train, sc_gene, sc_v = scale_features_in_fold(data_train, np.asarray(train_idx))\n",
    "    data_trainval = apply_scalers_to_data(sc_gene, sc_v, data_trainval)\n",
    "    data_full     = apply_scalers_to_data(sc_gene, sc_v, data_full)\n",
    "\n",
    "    # Check Ï†4 exists on train graph\n",
    "    phi_names_train = [k[1] for k in data_train.edge_index_dict.keys()\n",
    "                       if (k[0]=='gene' and k[2]=='gene' and k[1].startswith('phi') and data_train[k].edge_index.numel()>0)]\n",
    "    if 'phi4' not in phi_names_train:\n",
    "        raise RuntimeError(\"Ï†4 required but missing in TRAIN graph.\")\n",
    "\n",
    "    # Model\n",
    "    model = SimpleHAN(in_dim_gene=in_gene, in_dim_virus=in_virus,\n",
    "                      phi_names=phi_names_train,\n",
    "                      hidden=HIDDEN, heads=HEADS, dropout=DROPOUT,\n",
    "                      use_post_mha=True, use_virus_token=True).to(DEVICE)\n",
    "\n",
    "    model = train_one_fold_inductive(model, data_train, data_trainval,\n",
    "                                     np.asarray(train_idx), np.asarray(val_idx),\n",
    "                                     lr=LR, max_epochs=MAX_EPOCHS, weight_decay=WEIGHT_DECAY,\n",
    "                                     patience=PATIENCE)\n",
    "\n",
    "    # Prob on validation (trainâˆªval graph)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_val = model(data_trainval.x_dict, data_trainval.edge_index_dict)['gene'][torch.as_tensor(val_idx, device=DEVICE)]\n",
    "        prob_val   = torch.sigmoid(logits_val).detach().cpu().numpy()\n",
    "\n",
    "    # Prob on test (full graph)\n",
    "    test_res, prob_test, _ = evaluate_split(model, data_full, np.asarray(test_idx), thr=None)\n",
    "    # Here we need probs, not thresholding\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(data_full.x_dict, data_full.edge_index_dict)['gene'][torch.as_tensor(test_idx, device=DEVICE)]\n",
    "        prob_test   = torch.sigmoid(logits_test).detach().cpu().numpy()\n",
    "\n",
    "    return prob_val, prob_test\n",
    "\n",
    "\n",
    "def build_full_graph_scaled_later(data_base):\n",
    "    data_full = add_budget_relations(copy_like(data_base))\n",
    "    data_full = add_metapath_phi_edges(data_full, include_vv=INCLUDE_VV_METAPATH, min_vv_edges=MIN_VV_EDGES_FOR_USE)\n",
    "    return data_full\n",
    "\n",
    "def copy_like(data):\n",
    "    import copy as _copy\n",
    "    return _copy.deepcopy(data)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RF + HAN â†’ OOF VAL + TEST per fold\n",
    "# -------------------------\n",
    "def make_splits(y: np.ndarray, scheme: str = SPLIT_SCHEME, seed: int = SEED):\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "    for fold, (train_pool, test_idx) in enumerate(skf.split(np.zeros_like(y), y), 1):\n",
    "        if scheme == \"han_5x5\":\n",
    "            # split held-out test_idx 50/50 into val/test\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=seed)\n",
    "            te_labels = y[test_idx]\n",
    "            val_rel, test_rel = next(sss.split(np.zeros_like(te_labels), te_labels))\n",
    "            val_idx = test_idx[val_rel]\n",
    "            test2_idx = test_idx[test_rel]\n",
    "            train_idx = train_pool\n",
    "        else:\n",
    "            # 8:1:1 â†’ test = held-out fold (10%), val = 10% of ALL chosen from the 90% train pool\n",
    "            train_full = train_pool\n",
    "            # val fraction inside the 90% pool to reach 10% of ALL\n",
    "            val_frac_in_pool = 0.1 / 0.9\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=val_frac_in_pool, random_state=seed)\n",
    "            tr_labels = y[train_full]\n",
    "            tr_rel, va_rel = next(sss.split(np.zeros_like(tr_labels), tr_labels))\n",
    "            train_idx = train_full[tr_rel]\n",
    "            val_idx   = train_full[va_rel]\n",
    "            test2_idx = test_idx\n",
    "        yield fold, train_idx, val_idx, test2_idx\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load final full predictions for ALL genes (for deployment)\n",
    "# -------------------------\n",
    "import re, glob, gzip\n",
    "\n",
    "def load_han_all_preds(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Expect: Gene_Name | Predicted_Probability\n",
    "    gene_col = None\n",
    "    for cand in ['Gene_Name','Genes','Gene','Ensembl_ID']:\n",
    "        if cand in df.columns: gene_col = cand; break\n",
    "    pcol = None\n",
    "    for cand in ['Predicted_Probability','Prob','Probability','Prob_HDF','Score']:\n",
    "        if cand in df.columns: pcol = cand; break\n",
    "    if gene_col is None or pcol is None:\n",
    "        raise RuntimeError(\"Columns for 'gene id' and 'prob' not found in HAN final CSV.\")\n",
    "    out = df[[gene_col, pcol]].copy()\n",
    "    out.columns = ['Ensembl_ID','Prob_HAN']\n",
    "    return out\n",
    "\n",
    "def load_rf_all_preds(pred_dir: str, keyword: Optional[str] = None) -> pd.DataFrame:\n",
    "    pats = glob.glob(os.path.join(pred_dir, \"*.tsv.gz\"))\n",
    "    if keyword:\n",
    "        pats = [p for p in pats if re.search(keyword, os.path.basename(p), flags=re.IGNORECASE)]\n",
    "    if not pats:\n",
    "        raise RuntimeError(f\"No RF prediction files in {pred_dir}.\")\n",
    "    dfs = []\n",
    "    for p in pats:\n",
    "        with gzip.open(p, 'rt') as f:\n",
    "            d = pd.read_csv(f, sep='\\t')\n",
    "        need = ['Ensembl_ID','Prob_HDF']\n",
    "        if not set(need).issubset(d.columns): continue\n",
    "        d = d[need].rename(columns={'Prob_HDF': f\"Prob_RF_{os.path.splitext(os.path.basename(p))[0]}\"})\n",
    "        dfs.append(d)\n",
    "    base = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        base = base.merge(d, on='Ensembl_ID', how='outer')\n",
    "    rf_cols = [c for c in base.columns if c.startswith(\"Prob_RF_\")]\n",
    "    base['Prob_RF'] = base[rf_cols].mean(axis=1)\n",
    "    return base[['Ensembl_ID','Prob_RF']].copy()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    set_seeds(SEED)\n",
    "\n",
    "    # ----- 0) Labeled data (for RF features + labels)\n",
    "    lab_short, lab_full = load_labeled_df(FILE_GENE)\n",
    "    gene_ids = lab_short['Ensembl_ID'].values.astype(str)\n",
    "    y = lab_short['y'].values.astype(int)\n",
    "\n",
    "    # RF feature matrix from FILE_GENE (all non-id/non-class columns)\n",
    "    X_rf = lab_full.drop(columns=['Genes','Class'])\n",
    "    X_rf.columns = [str(c) for c in X_rf.columns]\n",
    "    X_rf = X_rf.apply(pd.to_numeric, errors='coerce')\n",
    "    if X_rf.isna().any().any():\n",
    "        X_rf = X_rf.fillna(X_rf.median())\n",
    "\n",
    "    # ----- 1) Build base hetero graph for HAN (once)\n",
    "    data_base, genes_df, virus_df = build_base_heterodata(apply_undirected=True)\n",
    "    in_gene  = data_base['gene'].x.size(1)\n",
    "    in_virus = data_base['virus'].x.size(1)\n",
    "    # Sanity: ensure same gene order between data_base and lab file\n",
    "    # here we assume FILE_GENE order matches data_base['gene'] order.\n",
    "    # If not, you may map indices by name; for this dataset they are aligned.\n",
    "\n",
    "    # ----- 2) OOF containers\n",
    "    n = len(y)\n",
    "    oof_val_rf  = np.full(n, np.nan, dtype=float)\n",
    "    oof_val_han = np.full(n, np.nan, dtype=float)\n",
    "    oof_val_y   = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    test_rows = []  # will collect rows for all folds: {'idx':..,'fold':..,'prob_rf':..,'prob_han':..,'y':..}\n",
    "\n",
    "    # ----- 3) Iterate folds (shared splits)\n",
    "    for fold, train_idx, val_idx, test_idx in make_splits(y, scheme=SPLIT_SCHEME, seed=SEED):\n",
    "        print(f\"[Fold {fold}] sizes -> train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "\n",
    "        # RF per fold\n",
    "        prob_rf_val, prob_rf_test = train_rf_fold_return_probs(X_rf, y, train_idx, val_idx, test_idx)\n",
    "\n",
    "        # HAN per fold\n",
    "        prob_han_val, prob_han_test = han_fold_return_probs(\n",
    "            data_base, in_gene, in_virus, y,\n",
    "            train_idx, val_idx, test_idx, seed=SEED\n",
    "        )\n",
    "\n",
    "        # Fill OOF val\n",
    "        oof_val_rf[val_idx]  = prob_rf_val\n",
    "        oof_val_han[val_idx] = prob_han_val\n",
    "        oof_val_y[val_idx]   = y[val_idx]\n",
    "\n",
    "        # Collect test rows\n",
    "        for i, idx in enumerate(test_idx):\n",
    "            test_rows.append({\n",
    "                'Index': int(idx),\n",
    "                'Fold': int(fold),\n",
    "                'Ensembl_ID': gene_ids[idx],\n",
    "                'y': int(y[idx]),\n",
    "                'Prob_RF': float(prob_rf_test[i]),\n",
    "                'Prob_HAN': float(prob_han_test[i]),\n",
    "            })\n",
    "\n",
    "    # ----- 4) Save OOF val table & test table\n",
    "    oof_val_df = pd.DataFrame({\n",
    "        'Index': np.arange(n, dtype=int),\n",
    "        'Ensembl_ID': gene_ids,\n",
    "        'y': oof_val_y,\n",
    "        'Prob_RF': oof_val_rf,\n",
    "        'Prob_HAN': oof_val_han\n",
    "    }).dropna(subset=['y','Prob_RF','Prob_HAN'])\n",
    "    oof_val_df.to_csv(os.path.join(OUT_DIR, \"oof_val_table.csv\"), index=False)\n",
    "\n",
    "    test_df = pd.DataFrame(test_rows)\n",
    "    test_df = test_df.sort_values(['Fold','Index']).reset_index(drop=True)\n",
    "    test_df.to_csv(os.path.join(OUT_DIR, \"perfold_test_table.csv\"), index=False)\n",
    "\n",
    "    # ----- 5) Fit stacking meta-learner on OOF(val), evaluate on test\n",
    "    X_val = oof_val_df[['Prob_RF','Prob_HAN']].values\n",
    "    y_val = oof_val_df['y'].values.astype(int)\n",
    "\n",
    "    # choose C by inner CV\n",
    "    Cs = [0.01, 0.1, 1, 3, 10, 30]\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_auc, best_C = -1.0, 1.0\n",
    "    for C in Cs:\n",
    "        aucs=[]\n",
    "        for tr, va in skf.split(X_val, y_val):\n",
    "            lr = LogisticRegression(C=C, solver='liblinear')\n",
    "            lr.fit(X_val[tr], y_val[tr])\n",
    "            p = lr.predict_proba(X_val[va])[:,1]\n",
    "            aucs.append(roc_auc_score(y_val[va], p))\n",
    "        m = float(np.mean(aucs))\n",
    "        if m > best_auc:\n",
    "            best_auc, best_C = m, C\n",
    "    meta = LogisticRegression(C=best_C, solver='liblinear')\n",
    "    meta.fit(X_val, y_val)\n",
    "\n",
    "    # Evaluate on test (concatenate all folds)\n",
    "    test_mat = test_df[['Prob_RF','Prob_HAN']].values\n",
    "    test_prob_ens = meta.predict_proba(test_mat)[:,1]\n",
    "\n",
    "    m_base_rf  = evaluate_metrics_bin(test_df['y'].values, test_df['Prob_RF'].values)\n",
    "    m_base_han = evaluate_metrics_bin(test_df['y'].values, test_df['Prob_HAN'].values)\n",
    "    m_ens      = evaluate_metrics_bin(test_df['y'].values, test_prob_ens)\n",
    "\n",
    "    met_tbl = pd.DataFrame([\n",
    "        {'Model':'RF',  **m_base_rf},\n",
    "        {'Model':'HAN', **m_base_han},\n",
    "        {'Model':'Stacking (LR, OOFâ†’meta)', **m_ens},\n",
    "    ])\n",
    "    met_tbl.to_csv(os.path.join(OUT_DIR, \"stacking_metrics_test.csv\"), index=False)\n",
    "\n",
    "    info = {\n",
    "        'scheme': SPLIT_SCHEME,\n",
    "        'n_splits': N_SPLITS,\n",
    "        'seed': SEED,\n",
    "        'meta_best_C': best_C,\n",
    "        'meta_cv_auc_on_OOF_val': best_auc\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"stacking_meta_info.json\"), 'w') as f:\n",
    "        json.dump(info, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Test metrics (aggregated across folds) ===\")\n",
    "    print(met_tbl.to_string(index=False))\n",
    "\n",
    "    # ----- 6) Apply ensemble to ALL genes (final HAN+RF global preds)\n",
    "    print(\"\\nApplying ensemble to ALL genes...\")\n",
    "    HAN_all = load_han_all_preds(HAN_PRED_CSV)\n",
    "    RF_all  = load_rf_all_preds(RF_PRED_DIR, keyword=RF_FILENAME_FILTER_KEYWORD)\n",
    "    ALL = pd.merge(HAN_all, RF_all, on='Ensembl_ID', how='inner').dropna()\n",
    "    if ALL.empty:\n",
    "        warnings.warn(\"No overlap between HAN and RF all-gene predictions.\")\n",
    "    X_all = ALL[['Prob_RF','Prob_HAN']].values\n",
    "    ALL['Prob_Ensemble'] = meta.predict_proba(X_all)[:,1]\n",
    "    ALL = ALL.sort_values('Prob_Ensemble', ascending=False).reset_index(drop=True)\n",
    "    ALL.to_csv(os.path.join(OUT_DIR, \"ensemble_predictions_all_genes.csv\"), index=False)\n",
    "    print(f\"Saved: {os.path.join(OUT_DIR, 'ensemble_predictions_all_genes.csv')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
